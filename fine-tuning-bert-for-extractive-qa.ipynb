{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/susantaghosh/fine-tuning-bert-for-extractive-qa?scriptVersionId=96924287\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/susantaghosh1/nlp-notebooks/blob/develop/Fine_Tuning_Extractive_QA_with_BERT_and_Friends.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"# Fine Tuning BERT/RoBERTa/DeBERTa/ALBERT/DistillBERT for extractive QA on Squad dataset","metadata":{"id":"UWOCThD20H8r"}},{"cell_type":"markdown","source":"In this section we will fine-tune Extractive QA on Squad dataset. Encoder-only models like BERT tend to be great at extracting answers to factoid questions like “Who invented the Transformer architecture?” but fare poorly when given open-ended questions like “Why is the sky blue?” In these more challenging cases, encoder-decoder models like T5 and BART are typically used to synthesize the information in a way that’s quite similar to text summarization.","metadata":{"id":"-IdJqLwX0qNr"}},{"cell_type":"markdown","source":"All of those work in the same way: they add a linear layer on top of the base model, which is used to produce a tensor of shape (batch_size,sequence_length,2), indicating the unnormalized scores **[LOGITS]** for start position and end position of the answers for every example in the batch.","metadata":{"id":"SSP9JjD9059z"}},{"cell_type":"markdown","source":"Let's discuss little bit internal working of the model :\n\n1. Question and Context [tokenized version] will be passed together as a pair to the model **[ let's say shape of input to the model is (5,30) where 5 is batch_size and 30 is sequence length [number of tokens in each input]**\n2. Vanilla BERT [OR it's friends] will produce contextualized embeddings for each and every word in the sequence. Shape of output from BERT is **(5,30,768) where 5 is the batch size, 30 is the sequece length and 768 is the embedding dimension of the each token**\n3. Now a linear head will be added on top of each of the tokens and each liner layer will take 768 dim as input and outputs 2 tensors , which we call start_logits and end_logits. Now, shape of output is **(5,30,2)**\n4. Now we will split the start_logits and end_logits where shape of each logits are **(5,30,1)**\n5. Now we will remove the single dimesion from the last dimension of start and end logits or in other words we will squeeze the start and end logits across the last dimesion and now shape of start and end logits will be **(5,30)**\n\n**start_logits = tensor of shape (5,30)**\n**end_logits = tensor of shape (5,30)**\n\n6. Model will take start_positions and end_positions of the answer in the tokenized data as labels\n\nstart_positions (`torch.LongTensor` of shape `(batch_size,)`):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence are not taken into account for computing the loss.\n\nend_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence are not taken into account for computing the loss.\n\n**start_positions = tensor of shape (5,)**\n**end_positions = tensor of shape (5,)**\n\n7. Now Cross Entropy loss will be computed between **start_logits and start_positions** and **end_logits and end_positions**.\n\n8. Total loss will be the average loss of **start_logits and start_positions** and end_logits and end_positions** and it will be backpropagated to the model for calculationg the gradients and optimizing the weights\n\nPseudo code for QA Model with BERT\n\nclass PseudoQA(nn.Module):\n\n  def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n  \n   def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        start_positions: Optional[torch.Tensor] = None,\n        end_positions: Optional[torch.Tensor] = None,\n    ) :\n        \n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0] ## ** last hidden state output of bert**\n\n        # ** shape of sequence_output : (batch_size,sequence_length,768) **\n\n        logits = self.qa_outputs(sequence_output)\n        # ** shape of logits : (batch_size,sequence_length,2) **\n        start_logits, end_logits = logits.split(1, dim=-1)\n        # ** shape of start_logits and end_logits : (batch_size,sequence_length,1) **\n        start_logits = start_logits.squeeze(-1).contiguous() # ** shape : (batch_size,sequence_length) **\n        end_logits = end_logits.squeeze(-1).contiguous() # ** shape : (batch_size,sequence_length) **\n\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, \n            # we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions = start_positions.clamp(0, ignored_index)\n            end_positions = end_positions.clamp(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n  \n\n","metadata":{"id":"rNXXtkju2Nni"}},{"cell_type":"markdown","source":"Enough of theory!!!! Let's dirty our hands","metadata":{"id":"_bBJUi-iNAuG"}},{"cell_type":"code","source":"%%capture\n!pip install datasets transformers[sentencepiece]\n!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n!pip install scipy sklearn","metadata":{"id":"8wtg2T5g6Epj","execution":{"iopub.status.busy":"2022-05-29T11:29:56.411427Z","iopub.execute_input":"2022-05-29T11:29:56.411863Z","iopub.status.idle":"2022-05-29T11:30:28.452634Z","shell.execute_reply.started":"2022-05-29T11:29:56.411781Z","shell.execute_reply":"2022-05-29T11:30:28.451449Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\ndevice","metadata":{"id":"fRNa7Cx0M8_W","outputId":"88704b81-10f6-4344-d6b9-03091c15c0fe","execution":{"iopub.status.busy":"2022-05-29T11:30:28.455069Z","iopub.execute_input":"2022-05-29T11:30:28.455432Z","iopub.status.idle":"2022-05-29T11:30:30.43356Z","shell.execute_reply.started":"2022-05-29T11:30:28.455395Z","shell.execute_reply":"2022-05-29T11:30:30.432595Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"ZyPHX3ByM_jd","outputId":"222e5dc5-da10-4b12-a714-059c838a5142","execution":{"iopub.status.busy":"2022-05-29T11:30:30.435512Z","iopub.execute_input":"2022-05-29T11:30:30.436617Z","iopub.status.idle":"2022-05-29T11:30:31.184589Z","shell.execute_reply.started":"2022-05-29T11:30:30.43657Z","shell.execute_reply":"2022-05-29T11:30:31.183449Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Sun May 29 11:30:31 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   38C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"# load the dataset\n\nfrom datasets import load_dataset\n\nraw_datasets = load_dataset(\"squad\")","metadata":{"id":"yF5FW2LuP4ZN","outputId":"f226385b-23cd-4454-f8cc-93601a16ad49","execution":{"iopub.status.busy":"2022-05-29T11:30:31.187943Z","iopub.execute_input":"2022-05-29T11:30:31.188385Z","iopub.status.idle":"2022-05-29T11:30:49.983579Z","shell.execute_reply.started":"2022-05-29T11:30:31.188338Z","shell.execute_reply":"2022-05-29T11:30:49.982675Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.97k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06520f43f1ac48179bfd799740a61c20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9536a1d515c4477e8bf73dbf73a32646"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da415197298f4cd5b41dec53a482bf7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/8.12M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0202eaac5fb24c59ae4f36660df55538"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.05M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74647be4efe646558f299302521c4a4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"846a9757ec9c43b3be87b9c53f98a258"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15fcdacee9714ccda5fa23f76c3d2c8f"}},"metadata":{}}]},{"cell_type":"code","source":"raw_datasets","metadata":{"id":"EZ43kFkGQB92","outputId":"234e45c7-6549-4295-ed35-379e82259460","execution":{"iopub.status.busy":"2022-05-29T11:30:49.986103Z","iopub.execute_input":"2022-05-29T11:30:49.987064Z","iopub.status.idle":"2022-05-29T11:30:49.995035Z","shell.execute_reply.started":"2022-05-29T11:30:49.98702Z","shell.execute_reply":"2022-05-29T11:30:49.994116Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 87599\n    })\n    validation: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 10570\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(\"Context: \", raw_datasets[\"train\"][0][\"context\"])\nprint(\"Question: \", raw_datasets[\"train\"][0][\"question\"])\nprint(\"Answer: \", raw_datasets[\"train\"][0][\"answers\"])","metadata":{"id":"WmtmryzAQG5e","outputId":"bb0f30e0-9505-4f1f-b772-4839d5f9771f","execution":{"iopub.status.busy":"2022-05-29T11:30:49.996414Z","iopub.execute_input":"2022-05-29T11:30:49.997431Z","iopub.status.idle":"2022-05-29T11:30:50.086318Z","shell.execute_reply.started":"2022-05-29T11:30:49.997389Z","shell.execute_reply":"2022-05-29T11:30:50.085234Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Context:  Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\nQuestion:  To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\nAnswer:  {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n","output_type":"stream"}]},{"cell_type":"code","source":"print(raw_datasets[\"train\"][0][\"answers\"].keys())\nprint(type(raw_datasets[\"train\"][0][\"answers\"]['text']))\nprint(raw_datasets[\"train\"][0][\"answers\"]['text'][0])","metadata":{"id":"yvnINZheQWU3","outputId":"0056e6fb-cf78-4110-8efd-60ec91c345fb","execution":{"iopub.status.busy":"2022-05-29T11:30:50.088119Z","iopub.execute_input":"2022-05-29T11:30:50.089218Z","iopub.status.idle":"2022-05-29T11:30:50.097488Z","shell.execute_reply.started":"2022-05-29T11:30:50.089178Z","shell.execute_reply":"2022-05-29T11:30:50.096378Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"dict_keys(['text', 'answer_start'])\n<class 'list'>\nSaint Bernadette Soubirous\n","output_type":"stream"}]},{"cell_type":"code","source":"answer = raw_datasets[\"train\"][0][\"answers\"]['text'][0]\nanswer_start = raw_datasets[\"train\"][0][\"answers\"]['answer_start'][0]\nanswer_end = answer_start + len(answer)\nanswer_from_context = raw_datasets[\"train\"][0][\"context\"] [answer_start:answer_end]\n","metadata":{"id":"LdlXXwrnQtmG","execution":{"iopub.status.busy":"2022-05-29T11:30:50.099256Z","iopub.execute_input":"2022-05-29T11:30:50.09973Z","iopub.status.idle":"2022-05-29T11:30:50.107758Z","shell.execute_reply.started":"2022-05-29T11:30:50.099685Z","shell.execute_reply":"2022-05-29T11:30:50.106856Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"answer_from_context","metadata":{"id":"58sejr9eRPYw","outputId":"3665f685-8d37-4b2c-87b6-42eb045fb7bb","execution":{"iopub.status.busy":"2022-05-29T11:30:50.109237Z","iopub.execute_input":"2022-05-29T11:30:50.110233Z","iopub.status.idle":"2022-05-29T11:30:50.118052Z","shell.execute_reply.started":"2022-05-29T11:30:50.110193Z","shell.execute_reply":"2022-05-29T11:30:50.117075Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'Saint Bernadette Soubirous'"},"metadata":{}}]},{"cell_type":"markdown","source":"During training, there is only one possible answer. We can double-check this by using the Dataset.filter() method:","metadata":{"id":"-d-TVnBFTL0u"}},{"cell_type":"code","source":"raw_datasets[\"train\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)","metadata":{"id":"o1Ut4701TMxO","outputId":"8dc73478-af8a-4291-adab-469fc275ac91","execution":{"iopub.status.busy":"2022-05-29T11:30:50.120459Z","iopub.execute_input":"2022-05-29T11:30:50.121125Z","iopub.status.idle":"2022-05-29T11:30:52.681932Z","shell.execute_reply.started":"2022-05-29T11:30:50.121082Z","shell.execute_reply":"2022-05-29T11:30:52.681039Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/88 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f9239b15b294ffd97a7aebffa79516d"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'title', 'context', 'question', 'answers'],\n    num_rows: 0\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"For evaluation, however, there are several possible answers for each sample, which may be the same or different:","metadata":{"id":"bfbqU4vJTZhm"}},{"cell_type":"code","source":"print(raw_datasets[\"validation\"][0][\"answers\"])\nprint(raw_datasets[\"validation\"][2][\"answers\"])","metadata":{"id":"2qLBYycwTa8A","outputId":"f2599b0e-60a7-4635-ae5a-a7df7ebb140b","execution":{"iopub.status.busy":"2022-05-29T11:30:52.683415Z","iopub.execute_input":"2022-05-29T11:30:52.683972Z","iopub.status.idle":"2022-05-29T11:30:52.690463Z","shell.execute_reply.started":"2022-05-29T11:30:52.683914Z","shell.execute_reply":"2022-05-29T11:30:52.6896Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'], 'answer_start': [177, 177, 177]}\n{'text': ['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"], 'answer_start': [403, 355, 355]}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# PreProcessing the training data","metadata":{"id":"xM--6qJeTqOf"}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_checkpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"id":"2BktODGOTxon","outputId":"ddbf8d1f-2931-4caf-ec3e-308b4817bde8","execution":{"iopub.status.busy":"2022-05-29T11:30:52.691909Z","iopub.execute_input":"2022-05-29T11:30:52.692475Z","iopub.status.idle":"2022-05-29T11:30:54.692115Z","shell.execute_reply.started":"2022-05-29T11:30:52.692435Z","shell.execute_reply":"2022-05-29T11:30:54.69115Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70ca412a8eec4b85878f7e7da7cf4783"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee5c2ef43c843d3a59de2613636b3a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f738670945b41e0b701c6c7d331a079"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ffaf3270cd641ff809997971e29a1e9"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.is_fast,tokenizer.special_tokens_map","metadata":{"id":"yD-6GHL-T23H","outputId":"549dfe47-4711-46dc-ed03-e025e709669f","execution":{"iopub.status.busy":"2022-05-29T11:30:54.693419Z","iopub.execute_input":"2022-05-29T11:30:54.693807Z","iopub.status.idle":"2022-05-29T11:30:54.702086Z","shell.execute_reply.started":"2022-05-29T11:30:54.693769Z","shell.execute_reply":"2022-05-29T11:30:54.701269Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(True,\n {'unk_token': '[UNK]',\n  'sep_token': '[SEP]',\n  'pad_token': '[PAD]',\n  'cls_token': '[CLS]',\n  'mask_token': '[MASK]'})"},"metadata":{}}]},{"cell_type":"markdown","source":"We can pass to our tokenizer the question and the context together, and it will properly insert the special tokens to form a sentence like this:\n\nCopied\n[CLS] question [SEP] context [SEP]","metadata":{"id":"p34OEOUMUA_X"}},{"cell_type":"markdown","source":"a predicted answer to all the acceptable answers and take the best score. ","metadata":{"id":"mzFGnoH6TlpQ"}},{"cell_type":"markdown","source":"","metadata":{"id":"SsG8T3v3amQX"}},{"cell_type":"code","source":"context = raw_datasets[\"train\"][0][\"context\"]\nquestion = raw_datasets[\"train\"][0][\"question\"]\n\ninputs = tokenizer(question, context,return_offsets_mapping=True)\n","metadata":{"id":"FMfQjUUDUDaH","execution":{"iopub.status.busy":"2022-05-29T11:30:54.706903Z","iopub.execute_input":"2022-05-29T11:30:54.707596Z","iopub.status.idle":"2022-05-29T11:30:54.808269Z","shell.execute_reply.started":"2022-05-29T11:30:54.707553Z","shell.execute_reply":"2022-05-29T11:30:54.807353Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"len(inputs['input_ids']),len(inputs['offset_mapping']),inputs","metadata":{"id":"CEBR0FpJVF9L","outputId":"9c14a329-3f2a-47d5-f7e7-aab822b373c2","execution":{"iopub.status.busy":"2022-05-29T11:30:54.810439Z","iopub.execute_input":"2022-05-29T11:30:54.81113Z","iopub.status.idle":"2022-05-29T11:30:54.819257Z","shell.execute_reply.started":"2022-05-29T11:30:54.811089Z","shell.execute_reply":"2022-05-29T11:30:54.818265Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"(176,\n 176,\n {'input_ids': [101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102, 6549, 2135, 1010, 1996, 2082, 2038, 1037, 3234, 2839, 1012, 10234, 1996, 2364, 2311, 1005, 1055, 2751, 8514, 2003, 1037, 3585, 6231, 1997, 1996, 6261, 2984, 1012, 3202, 1999, 2392, 1997, 1996, 2364, 2311, 1998, 5307, 2009, 1010, 2003, 1037, 6967, 6231, 1997, 4828, 2007, 2608, 2039, 14995, 6924, 2007, 1996, 5722, 1000, 2310, 3490, 2618, 4748, 2033, 18168, 5267, 1000, 1012, 2279, 2000, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 23052, 1010, 1037, 14042, 2173, 1997, 7083, 1998, 9185, 1012, 2009, 2003, 1037, 15059, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271, 1999, 8517, 1012, 2012, 1996, 2203, 1997, 1996, 2364, 3298, 1006, 1998, 1999, 1037, 3622, 2240, 2008, 8539, 2083, 1017, 11342, 1998, 1996, 2751, 8514, 1007, 1010, 2003, 1037, 3722, 1010, 2715, 2962, 6231, 1997, 2984, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 252), (252, 254), (254, 256), (257, 259), (260, 262), (263, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 365), (366, 368), (369, 372), (373, 375), (375, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 447), (447, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 536), (536, 539), (539, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0)]})"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.decode(inputs[\"input_ids\"])\n","metadata":{"id":"uO3b74dQUZlX","outputId":"b6c2379b-faa3-4b9a-a472-49e703c4c8d6","execution":{"iopub.status.busy":"2022-05-29T11:30:54.8209Z","iopub.execute_input":"2022-05-29T11:30:54.821995Z","iopub.status.idle":"2022-05-29T11:30:56.453375Z","shell.execute_reply.started":"2022-05-29T11:30:54.821951Z","shell.execute_reply":"2022-05-29T11:30:56.452564Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'[CLS] to whom did the virgin mary allegedly appear in 1858 in lourdes france? [SEP] architecturally, the school has a catholic character. atop the main building\\'s gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ), is a simple, modern stone statue of mary. [SEP]'"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])","metadata":{"id":"169dwirTUwLf","outputId":"6dc9c854-2839-4dc2-a745-ed70fb017af6","execution":{"iopub.status.busy":"2022-05-29T11:30:56.455031Z","iopub.execute_input":"2022-05-29T11:30:56.455794Z","iopub.status.idle":"2022-05-29T11:30:56.464992Z","shell.execute_reply.started":"2022-05-29T11:30:56.455757Z","shell.execute_reply":"2022-05-29T11:30:56.464119Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"['[CLS]',\n 'to',\n 'whom',\n 'did',\n 'the',\n 'virgin',\n 'mary',\n 'allegedly',\n 'appear',\n 'in',\n '1858',\n 'in',\n 'lou',\n '##rdes',\n 'france',\n '?',\n '[SEP]',\n 'architectural',\n '##ly',\n ',',\n 'the',\n 'school',\n 'has',\n 'a',\n 'catholic',\n 'character',\n '.',\n 'atop',\n 'the',\n 'main',\n 'building',\n \"'\",\n 's',\n 'gold',\n 'dome',\n 'is',\n 'a',\n 'golden',\n 'statue',\n 'of',\n 'the',\n 'virgin',\n 'mary',\n '.',\n 'immediately',\n 'in',\n 'front',\n 'of',\n 'the',\n 'main',\n 'building',\n 'and',\n 'facing',\n 'it',\n ',',\n 'is',\n 'a',\n 'copper',\n 'statue',\n 'of',\n 'christ',\n 'with',\n 'arms',\n 'up',\n '##rai',\n '##sed',\n 'with',\n 'the',\n 'legend',\n '\"',\n 've',\n '##ni',\n '##te',\n 'ad',\n 'me',\n 'om',\n '##nes',\n '\"',\n '.',\n 'next',\n 'to',\n 'the',\n 'main',\n 'building',\n 'is',\n 'the',\n 'basilica',\n 'of',\n 'the',\n 'sacred',\n 'heart',\n '.',\n 'immediately',\n 'behind',\n 'the',\n 'basilica',\n 'is',\n 'the',\n 'gr',\n '##otto',\n ',',\n 'a',\n 'marian',\n 'place',\n 'of',\n 'prayer',\n 'and',\n 'reflection',\n '.',\n 'it',\n 'is',\n 'a',\n 'replica',\n 'of',\n 'the',\n 'gr',\n '##otto',\n 'at',\n 'lou',\n '##rdes',\n ',',\n 'france',\n 'where',\n 'the',\n 'virgin',\n 'mary',\n 'reputed',\n '##ly',\n 'appeared',\n 'to',\n 'saint',\n 'bern',\n '##ade',\n '##tte',\n 'so',\n '##ub',\n '##iro',\n '##us',\n 'in',\n '1858',\n '.',\n 'at',\n 'the',\n 'end',\n 'of',\n 'the',\n 'main',\n 'drive',\n '(',\n 'and',\n 'in',\n 'a',\n 'direct',\n 'line',\n 'that',\n 'connects',\n 'through',\n '3',\n 'statues',\n 'and',\n 'the',\n 'gold',\n 'dome',\n ')',\n ',',\n 'is',\n 'a',\n 'simple',\n ',',\n 'modern',\n 'stone',\n 'statue',\n 'of',\n 'mary',\n '.',\n '[SEP]']"},"metadata":{}}]},{"cell_type":"markdown","source":"In this case the context is not too long, but some of the examples in the dataset have very long contexts that will exceed the maximum length we set (which is 384 in this case).  we will deal with long contexts by creating several training features from one sample of our dataset, with a sliding window between them.\n\nTo see how this works using the current example, we can limit the length to 100 and use a sliding window of 50 tokens. As a reminder, we use:\n\nmax_length to set the maximum length (here 100)\ntruncation=\"only_second\" to truncate the context (which is in the second position) when the question with its context is too long\nstride to set the number of overlapping tokens between two successive chunks (here 50)\nreturn_overflowing_tokens=True to let the tokenizer know we want the overflowing tokens\n\nreturn_offsets_mapping=True to get the positions of the tokens with respect to the input of the tokenizer [for sequence_id =0, position of question otherwise positions of context]","metadata":{"id":"WqU8pIIPbWyo"}},{"cell_type":"code","source":"batch_encoding = tokenizer(question,context,max_length=100,truncation=\"only_second\",stride=50,\n                           return_overflowing_tokens=True,return_offsets_mapping=True)","metadata":{"id":"WbB-14qsb21H","execution":{"iopub.status.busy":"2022-05-29T11:30:56.466568Z","iopub.execute_input":"2022-05-29T11:30:56.466945Z","iopub.status.idle":"2022-05-29T11:30:56.475535Z","shell.execute_reply.started":"2022-05-29T11:30:56.466909Z","shell.execute_reply":"2022-05-29T11:30:56.474511Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"batch_encoding.keys(),len(batch_encoding['input_ids'])","metadata":{"id":"trnC18RWc-w5","outputId":"ac04e23a-33e3-44d2-c2b4-73530f5656e9","execution":{"iopub.status.busy":"2022-05-29T11:30:56.478378Z","iopub.execute_input":"2022-05-29T11:30:56.478931Z","iopub.status.idle":"2022-05-29T11:30:56.485772Z","shell.execute_reply.started":"2022-05-29T11:30:56.478904Z","shell.execute_reply":"2022-05-29T11:30:56.484992Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping']),\n 4)"},"metadata":{}}]},{"cell_type":"code","source":"batch_encoding","metadata":{"id":"eYxd_MnpeIAS","outputId":"0e1476b4-c3ed-42cf-95ac-c20b041d0fe9","execution":{"iopub.status.busy":"2022-05-29T11:30:56.487032Z","iopub.execute_input":"2022-05-29T11:30:56.48765Z","iopub.status.idle":"2022-05-29T11:30:56.498491Z","shell.execute_reply.started":"2022-05-29T11:30:56.487612Z","shell.execute_reply":"2022-05-29T11:30:56.497525Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [[101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102, 6549, 2135, 1010, 1996, 2082, 2038, 1037, 3234, 2839, 1012, 10234, 1996, 2364, 2311, 1005, 1055, 2751, 8514, 2003, 1037, 3585, 6231, 1997, 1996, 6261, 2984, 1012, 3202, 1999, 2392, 1997, 1996, 2364, 2311, 1998, 5307, 2009, 1010, 2003, 1037, 6967, 6231, 1997, 4828, 2007, 2608, 2039, 14995, 6924, 2007, 1996, 5722, 1000, 2310, 3490, 2618, 4748, 2033, 18168, 5267, 1000, 1012, 2279, 2000, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 102], [101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102, 2364, 2311, 1998, 5307, 2009, 1010, 2003, 1037, 6967, 6231, 1997, 4828, 2007, 2608, 2039, 14995, 6924, 2007, 1996, 5722, 1000, 2310, 3490, 2618, 4748, 2033, 18168, 5267, 1000, 1012, 2279, 2000, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 23052, 1010, 1037, 14042, 2173, 1997, 7083, 1998, 9185, 1012, 2009, 2003, 1037, 15059, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 102], [101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 23052, 1010, 1037, 14042, 2173, 1997, 7083, 1998, 9185, 1012, 2009, 2003, 1037, 15059, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271, 1999, 8517, 1012, 2012, 1996, 2203, 1997, 1996, 2364, 3298, 1006, 1998, 1999, 1037, 3622, 2240, 2008, 8539, 2083, 1017, 11342, 1998, 1996, 2751, 8514, 102], [101, 2000, 3183, 2106, 1996, 6261, 2984, 9382, 3711, 1999, 8517, 1999, 10223, 26371, 2605, 1029, 102, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271, 1999, 8517, 1012, 2012, 1996, 2203, 1997, 1996, 2364, 3298, 1006, 1998, 1999, 1037, 3622, 2240, 2008, 8539, 2083, 1017, 11342, 1998, 1996, 2751, 8514, 1007, 1010, 2003, 1037, 3722, 1010, 2715, 2962, 6231, 1997, 2984, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'offset_mapping': [[(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 252), (252, 254), (254, 256), (257, 259), (260, 262), (263, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 365), (366, 368), (369, 372), (373, 375), (0, 0)], [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 252), (252, 254), (254, 256), (257, 259), (260, 262), (263, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 365), (366, 368), (369, 372), (373, 375), (375, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 447), (447, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (0, 0)], [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 365), (366, 368), (369, 372), (373, 375), (375, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 447), (447, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 536), (536, 539), (539, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (0, 0)], [(0, 0), (0, 2), (3, 7), (8, 11), (12, 15), (16, 22), (23, 27), (28, 37), (38, 44), (45, 47), (48, 52), (53, 55), (56, 59), (59, 63), (64, 70), (70, 71), (0, 0), (438, 440), (441, 444), (445, 447), (447, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 536), (536, 539), (539, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0)]], 'overflow_to_sample_mapping': [0, 0, 0, 0]}"},"metadata":{}}]},{"cell_type":"code","source":"batch_encoding['overflow_to_sample_mapping'] # one long context has been truncated to 4 samples","metadata":{"id":"B13HJSF1dO14","outputId":"af2a18ed-81fd-4560-c59b-672317501136","execution":{"iopub.status.busy":"2022-05-29T11:30:56.499478Z","iopub.execute_input":"2022-05-29T11:30:56.501748Z","iopub.status.idle":"2022-05-29T11:30:56.508531Z","shell.execute_reply.started":"2022-05-29T11:30:56.501703Z","shell.execute_reply":"2022-05-29T11:30:56.507561Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"[0, 0, 0, 0]"},"metadata":{}}]},{"cell_type":"code","source":"sequence_ids = batch_encoding.sequence_ids(0)\nsliced_text = \"\"\nfor idx,tokens,positions in zip(range(len(batch_encoding['input_ids'][0])),batch_encoding['input_ids'][0],batch_encoding['offset_mapping'][0]):\n  if sequence_ids[idx]==0:\n    sliced_text = question[positions[0]:positions[1]]\n  elif sequence_ids[idx]==1:\n    sliced_text = context[positions[0]:positions[1]]\n  print(f\"tokens :: {tokens} and decoed token :: {tokenizer.convert_ids_to_tokens(tokens)} and positions :: {positions} and sliced  text :: {sliced_text}\")  ## positions for special tokens will be (0,0)","metadata":{"id":"8FlXbGwpdYBq","outputId":"06c047b4-1f5e-4483-fc25-e07b93b38998","execution":{"iopub.status.busy":"2022-05-29T11:30:56.510344Z","iopub.execute_input":"2022-05-29T11:30:56.510986Z","iopub.status.idle":"2022-05-29T11:30:56.52155Z","shell.execute_reply.started":"2022-05-29T11:30:56.510947Z","shell.execute_reply":"2022-05-29T11:30:56.52054Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"tokens :: 101 and decoed token :: [CLS] and positions :: (0, 0) and sliced  text :: \ntokens :: 2000 and decoed token :: to and positions :: (0, 2) and sliced  text :: To\ntokens :: 3183 and decoed token :: whom and positions :: (3, 7) and sliced  text :: whom\ntokens :: 2106 and decoed token :: did and positions :: (8, 11) and sliced  text :: did\ntokens :: 1996 and decoed token :: the and positions :: (12, 15) and sliced  text :: the\ntokens :: 6261 and decoed token :: virgin and positions :: (16, 22) and sliced  text :: Virgin\ntokens :: 2984 and decoed token :: mary and positions :: (23, 27) and sliced  text :: Mary\ntokens :: 9382 and decoed token :: allegedly and positions :: (28, 37) and sliced  text :: allegedly\ntokens :: 3711 and decoed token :: appear and positions :: (38, 44) and sliced  text :: appear\ntokens :: 1999 and decoed token :: in and positions :: (45, 47) and sliced  text :: in\ntokens :: 8517 and decoed token :: 1858 and positions :: (48, 52) and sliced  text :: 1858\ntokens :: 1999 and decoed token :: in and positions :: (53, 55) and sliced  text :: in\ntokens :: 10223 and decoed token :: lou and positions :: (56, 59) and sliced  text :: Lou\ntokens :: 26371 and decoed token :: ##rdes and positions :: (59, 63) and sliced  text :: rdes\ntokens :: 2605 and decoed token :: france and positions :: (64, 70) and sliced  text :: France\ntokens :: 1029 and decoed token :: ? and positions :: (70, 71) and sliced  text :: ?\ntokens :: 102 and decoed token :: [SEP] and positions :: (0, 0) and sliced  text :: ?\ntokens :: 6549 and decoed token :: architectural and positions :: (0, 13) and sliced  text :: Architectural\ntokens :: 2135 and decoed token :: ##ly and positions :: (13, 15) and sliced  text :: ly\ntokens :: 1010 and decoed token :: , and positions :: (15, 16) and sliced  text :: ,\ntokens :: 1996 and decoed token :: the and positions :: (17, 20) and sliced  text :: the\ntokens :: 2082 and decoed token :: school and positions :: (21, 27) and sliced  text :: school\ntokens :: 2038 and decoed token :: has and positions :: (28, 31) and sliced  text :: has\ntokens :: 1037 and decoed token :: a and positions :: (32, 33) and sliced  text :: a\ntokens :: 3234 and decoed token :: catholic and positions :: (34, 42) and sliced  text :: Catholic\ntokens :: 2839 and decoed token :: character and positions :: (43, 52) and sliced  text :: character\ntokens :: 1012 and decoed token :: . and positions :: (52, 53) and sliced  text :: .\ntokens :: 10234 and decoed token :: atop and positions :: (54, 58) and sliced  text :: Atop\ntokens :: 1996 and decoed token :: the and positions :: (59, 62) and sliced  text :: the\ntokens :: 2364 and decoed token :: main and positions :: (63, 67) and sliced  text :: Main\ntokens :: 2311 and decoed token :: building and positions :: (68, 76) and sliced  text :: Building\ntokens :: 1005 and decoed token :: ' and positions :: (76, 77) and sliced  text :: '\ntokens :: 1055 and decoed token :: s and positions :: (77, 78) and sliced  text :: s\ntokens :: 2751 and decoed token :: gold and positions :: (79, 83) and sliced  text :: gold\ntokens :: 8514 and decoed token :: dome and positions :: (84, 88) and sliced  text :: dome\ntokens :: 2003 and decoed token :: is and positions :: (89, 91) and sliced  text :: is\ntokens :: 1037 and decoed token :: a and positions :: (92, 93) and sliced  text :: a\ntokens :: 3585 and decoed token :: golden and positions :: (94, 100) and sliced  text :: golden\ntokens :: 6231 and decoed token :: statue and positions :: (101, 107) and sliced  text :: statue\ntokens :: 1997 and decoed token :: of and positions :: (108, 110) and sliced  text :: of\ntokens :: 1996 and decoed token :: the and positions :: (111, 114) and sliced  text :: the\ntokens :: 6261 and decoed token :: virgin and positions :: (115, 121) and sliced  text :: Virgin\ntokens :: 2984 and decoed token :: mary and positions :: (122, 126) and sliced  text :: Mary\ntokens :: 1012 and decoed token :: . and positions :: (126, 127) and sliced  text :: .\ntokens :: 3202 and decoed token :: immediately and positions :: (128, 139) and sliced  text :: Immediately\ntokens :: 1999 and decoed token :: in and positions :: (140, 142) and sliced  text :: in\ntokens :: 2392 and decoed token :: front and positions :: (143, 148) and sliced  text :: front\ntokens :: 1997 and decoed token :: of and positions :: (149, 151) and sliced  text :: of\ntokens :: 1996 and decoed token :: the and positions :: (152, 155) and sliced  text :: the\ntokens :: 2364 and decoed token :: main and positions :: (156, 160) and sliced  text :: Main\ntokens :: 2311 and decoed token :: building and positions :: (161, 169) and sliced  text :: Building\ntokens :: 1998 and decoed token :: and and positions :: (170, 173) and sliced  text :: and\ntokens :: 5307 and decoed token :: facing and positions :: (174, 180) and sliced  text :: facing\ntokens :: 2009 and decoed token :: it and positions :: (181, 183) and sliced  text :: it\ntokens :: 1010 and decoed token :: , and positions :: (183, 184) and sliced  text :: ,\ntokens :: 2003 and decoed token :: is and positions :: (185, 187) and sliced  text :: is\ntokens :: 1037 and decoed token :: a and positions :: (188, 189) and sliced  text :: a\ntokens :: 6967 and decoed token :: copper and positions :: (190, 196) and sliced  text :: copper\ntokens :: 6231 and decoed token :: statue and positions :: (197, 203) and sliced  text :: statue\ntokens :: 1997 and decoed token :: of and positions :: (204, 206) and sliced  text :: of\ntokens :: 4828 and decoed token :: christ and positions :: (207, 213) and sliced  text :: Christ\ntokens :: 2007 and decoed token :: with and positions :: (214, 218) and sliced  text :: with\ntokens :: 2608 and decoed token :: arms and positions :: (219, 223) and sliced  text :: arms\ntokens :: 2039 and decoed token :: up and positions :: (224, 226) and sliced  text :: up\ntokens :: 14995 and decoed token :: ##rai and positions :: (226, 229) and sliced  text :: rai\ntokens :: 6924 and decoed token :: ##sed and positions :: (229, 232) and sliced  text :: sed\ntokens :: 2007 and decoed token :: with and positions :: (233, 237) and sliced  text :: with\ntokens :: 1996 and decoed token :: the and positions :: (238, 241) and sliced  text :: the\ntokens :: 5722 and decoed token :: legend and positions :: (242, 248) and sliced  text :: legend\ntokens :: 1000 and decoed token :: \" and positions :: (249, 250) and sliced  text :: \"\ntokens :: 2310 and decoed token :: ve and positions :: (250, 252) and sliced  text :: Ve\ntokens :: 3490 and decoed token :: ##ni and positions :: (252, 254) and sliced  text :: ni\ntokens :: 2618 and decoed token :: ##te and positions :: (254, 256) and sliced  text :: te\ntokens :: 4748 and decoed token :: ad and positions :: (257, 259) and sliced  text :: Ad\ntokens :: 2033 and decoed token :: me and positions :: (260, 262) and sliced  text :: Me\ntokens :: 18168 and decoed token :: om and positions :: (263, 265) and sliced  text :: Om\ntokens :: 5267 and decoed token :: ##nes and positions :: (265, 268) and sliced  text :: nes\ntokens :: 1000 and decoed token :: \" and positions :: (268, 269) and sliced  text :: \"\ntokens :: 1012 and decoed token :: . and positions :: (269, 270) and sliced  text :: .\ntokens :: 2279 and decoed token :: next and positions :: (271, 275) and sliced  text :: Next\ntokens :: 2000 and decoed token :: to and positions :: (276, 278) and sliced  text :: to\ntokens :: 1996 and decoed token :: the and positions :: (279, 282) and sliced  text :: the\ntokens :: 2364 and decoed token :: main and positions :: (283, 287) and sliced  text :: Main\ntokens :: 2311 and decoed token :: building and positions :: (288, 296) and sliced  text :: Building\ntokens :: 2003 and decoed token :: is and positions :: (297, 299) and sliced  text :: is\ntokens :: 1996 and decoed token :: the and positions :: (300, 303) and sliced  text :: the\ntokens :: 13546 and decoed token :: basilica and positions :: (304, 312) and sliced  text :: Basilica\ntokens :: 1997 and decoed token :: of and positions :: (313, 315) and sliced  text :: of\ntokens :: 1996 and decoed token :: the and positions :: (316, 319) and sliced  text :: the\ntokens :: 6730 and decoed token :: sacred and positions :: (320, 326) and sliced  text :: Sacred\ntokens :: 2540 and decoed token :: heart and positions :: (327, 332) and sliced  text :: Heart\ntokens :: 1012 and decoed token :: . and positions :: (332, 333) and sliced  text :: .\ntokens :: 3202 and decoed token :: immediately and positions :: (334, 345) and sliced  text :: Immediately\ntokens :: 2369 and decoed token :: behind and positions :: (346, 352) and sliced  text :: behind\ntokens :: 1996 and decoed token :: the and positions :: (353, 356) and sliced  text :: the\ntokens :: 13546 and decoed token :: basilica and positions :: (357, 365) and sliced  text :: basilica\ntokens :: 2003 and decoed token :: is and positions :: (366, 368) and sliced  text :: is\ntokens :: 1996 and decoed token :: the and positions :: (369, 372) and sliced  text :: the\ntokens :: 24665 and decoed token :: gr and positions :: (373, 375) and sliced  text :: Gr\ntokens :: 102 and decoed token :: [SEP] and positions :: (0, 0) and sliced  text :: Gr\n","output_type":"stream"}]},{"cell_type":"code","source":"# let's try to encode few more samples together\n\nsample_question =  raw_datasets[\"train\"][2:6][\"question\"] # list of size 4\nsample_context =  raw_datasets[\"train\"][2:6][\"context\"] # list of size 4\nsample_answers = raw_datasets[\"train\"][2:6][\"answers\"]\nsample_question,sample_question[0],sample_context[0],sample_answers[0]","metadata":{"id":"X6fFEkM7mbN4","outputId":"303cc061-4d09-417c-ee34-1f1a0303f0d7","execution":{"iopub.status.busy":"2022-05-29T11:30:56.523478Z","iopub.execute_input":"2022-05-29T11:30:56.523954Z","iopub.status.idle":"2022-05-29T11:30:56.535184Z","shell.execute_reply.started":"2022-05-29T11:30:56.52391Z","shell.execute_reply":"2022-05-29T11:30:56.534174Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"(['The Basilica of the Sacred heart at Notre Dame is beside to which structure?',\n  'What is the Grotto at Notre Dame?',\n  'What sits on top of the Main Building at Notre Dame?',\n  'When did the Scholastic Magazine of Notre dame begin publishing?'],\n 'The Basilica of the Sacred heart at Notre Dame is beside to which structure?',\n 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n {'text': ['the Main Building'], 'answer_start': [279]})"},"metadata":{}}]},{"cell_type":"code","source":"sample_encoding = tokenizer(sample_question,sample_context,max_length=100,truncation=\"only_second\",stride=50,\n                           return_overflowing_tokens=True,return_offsets_mapping=True)\nsample_encoding,sample_encoding.keys(),len(sample_encoding['offset_mapping'][0])","metadata":{"id":"Ub23TiWkm4Ao","outputId":"abb9e86a-1f7f-4abb-d7ef-106a4a5feb81","execution":{"iopub.status.busy":"2022-05-29T11:30:56.53742Z","iopub.execute_input":"2022-05-29T11:30:56.537979Z","iopub.status.idle":"2022-05-29T11:30:56.551964Z","shell.execute_reply.started":"2022-05-29T11:30:56.537941Z","shell.execute_reply":"2022-05-29T11:30:56.550779Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"({'input_ids': [[101, 1996, 13546, 1997, 1996, 6730, 2540, 2012, 10289, 8214, 2003, 3875, 2000, 2029, 3252, 1029, 102, 6549, 2135, 1010, 1996, 2082, 2038, 1037, 3234, 2839, 1012, 10234, 1996, 2364, 2311, 1005, 1055, 2751, 8514, 2003, 1037, 3585, 6231, 1997, 1996, 6261, 2984, 1012, 3202, 1999, 2392, 1997, 1996, 2364, 2311, 1998, 5307, 2009, 1010, 2003, 1037, 6967, 6231, 1997, 4828, 2007, 2608, 2039, 14995, 6924, 2007, 1996, 5722, 1000, 2310, 3490, 2618, 4748, 2033, 18168, 5267, 1000, 1012, 2279, 2000, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 102], [101, 1996, 13546, 1997, 1996, 6730, 2540, 2012, 10289, 8214, 2003, 3875, 2000, 2029, 3252, 1029, 102, 2364, 2311, 1998, 5307, 2009, 1010, 2003, 1037, 6967, 6231, 1997, 4828, 2007, 2608, 2039, 14995, 6924, 2007, 1996, 5722, 1000, 2310, 3490, 2618, 4748, 2033, 18168, 5267, 1000, 1012, 2279, 2000, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 23052, 1010, 1037, 14042, 2173, 1997, 7083, 1998, 9185, 1012, 2009, 2003, 1037, 15059, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 102], [101, 1996, 13546, 1997, 1996, 6730, 2540, 2012, 10289, 8214, 2003, 3875, 2000, 2029, 3252, 1029, 102, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 23052, 1010, 1037, 14042, 2173, 1997, 7083, 1998, 9185, 1012, 2009, 2003, 1037, 15059, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271, 1999, 8517, 1012, 2012, 1996, 2203, 1997, 1996, 2364, 3298, 1006, 1998, 1999, 1037, 3622, 2240, 2008, 8539, 2083, 1017, 11342, 1998, 1996, 2751, 8514, 102], [101, 1996, 13546, 1997, 1996, 6730, 2540, 2012, 10289, 8214, 2003, 3875, 2000, 2029, 3252, 1029, 102, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271, 1999, 8517, 1012, 2012, 1996, 2203, 1997, 1996, 2364, 3298, 1006, 1998, 1999, 1037, 3622, 2240, 2008, 8539, 2083, 1017, 11342, 1998, 1996, 2751, 8514, 1007, 1010, 2003, 1037, 3722, 1010, 2715, 2962, 6231, 1997, 2984, 1012, 102], [101, 2054, 2003, 1996, 24665, 23052, 2012, 10289, 8214, 1029, 102, 6549, 2135, 1010, 1996, 2082, 2038, 1037, 3234, 2839, 1012, 10234, 1996, 2364, 2311, 1005, 1055, 2751, 8514, 2003, 1037, 3585, 6231, 1997, 1996, 6261, 2984, 1012, 3202, 1999, 2392, 1997, 1996, 2364, 2311, 1998, 5307, 2009, 1010, 2003, 1037, 6967, 6231, 1997, 4828, 2007, 2608, 2039, 14995, 6924, 2007, 1996, 5722, 1000, 2310, 3490, 2618, 4748, 2033, 18168, 5267, 1000, 1012, 2279, 2000, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 23052, 1010, 1037, 14042, 2173, 1997, 102], [101, 2054, 2003, 1996, 24665, 23052, 2012, 10289, 8214, 1029, 102, 2003, 1037, 6967, 6231, 1997, 4828, 2007, 2608, 2039, 14995, 6924, 2007, 1996, 5722, 1000, 2310, 3490, 2618, 4748, 2033, 18168, 5267, 1000, 1012, 2279, 2000, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 23052, 1010, 1037, 14042, 2173, 1997, 7083, 1998, 9185, 1012, 2009, 2003, 1037, 15059, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271, 1999, 8517, 1012, 2012, 1996, 102], [101, 2054, 2003, 1996, 24665, 23052, 2012, 10289, 8214, 1029, 102, 2369, 1996, 13546, 2003, 1996, 24665, 23052, 1010, 1037, 14042, 2173, 1997, 7083, 1998, 9185, 1012, 2009, 2003, 1037, 15059, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271, 1999, 8517, 1012, 2012, 1996, 2203, 1997, 1996, 2364, 3298, 1006, 1998, 1999, 1037, 3622, 2240, 2008, 8539, 2083, 1017, 11342, 1998, 1996, 2751, 8514, 1007, 1010, 2003, 1037, 3722, 1010, 2715, 2962, 6231, 1997, 2984, 1012, 102], [101, 2054, 7719, 2006, 2327, 1997, 1996, 2364, 2311, 2012, 10289, 8214, 1029, 102, 6549, 2135, 1010, 1996, 2082, 2038, 1037, 3234, 2839, 1012, 10234, 1996, 2364, 2311, 1005, 1055, 2751, 8514, 2003, 1037, 3585, 6231, 1997, 1996, 6261, 2984, 1012, 3202, 1999, 2392, 1997, 1996, 2364, 2311, 1998, 5307, 2009, 1010, 2003, 1037, 6967, 6231, 1997, 4828, 2007, 2608, 2039, 14995, 6924, 2007, 1996, 5722, 1000, 2310, 3490, 2618, 4748, 2033, 18168, 5267, 1000, 1012, 2279, 2000, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 23052, 1010, 1037, 102], [101, 2054, 7719, 2006, 2327, 1997, 1996, 2364, 2311, 2012, 10289, 8214, 1029, 102, 5307, 2009, 1010, 2003, 1037, 6967, 6231, 1997, 4828, 2007, 2608, 2039, 14995, 6924, 2007, 1996, 5722, 1000, 2310, 3490, 2618, 4748, 2033, 18168, 5267, 1000, 1012, 2279, 2000, 1996, 2364, 2311, 2003, 1996, 13546, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 23052, 1010, 1037, 14042, 2173, 1997, 7083, 1998, 9185, 1012, 2009, 2003, 1037, 15059, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 16595, 9648, 4674, 2061, 12083, 9711, 102], [101, 2054, 7719, 2006, 2327, 1997, 1996, 2364, 2311, 2012, 10289, 8214, 1029, 102, 1997, 1996, 6730, 2540, 1012, 3202, 2369, 1996, 13546, 2003, 1996, 24665, 23052, 1010, 1037, 14042, 2173, 1997, 7083, 1998, 9185, 1012, 2009, 2003, 1037, 15059, 1997, 1996, 24665, 23052, 2012, 10223, 26371, 1010, 2605, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271, 1999, 8517, 1012, 2012, 1996, 2203, 1997, 1996, 2364, 3298, 1006, 1998, 1999, 1037, 3622, 2240, 2008, 8539, 2083, 1017, 11342, 1998, 1996, 2751, 8514, 1007, 1010, 2003, 1037, 3722, 1010, 2715, 2962, 6231, 102], [101, 2054, 7719, 2006, 2327, 1997, 1996, 2364, 2311, 2012, 10289, 8214, 1029, 102, 2073, 1996, 6261, 2984, 22353, 2135, 2596, 2000, 3002, 16595, 9648, 4674, 2061, 12083, 9711, 2271, 1999, 8517, 1012, 2012, 1996, 2203, 1997, 1996, 2364, 3298, 1006, 1998, 1999, 1037, 3622, 2240, 2008, 8539, 2083, 1017, 11342, 1998, 1996, 2751, 8514, 1007, 1010, 2003, 1037, 3722, 1010, 2715, 2962, 6231, 1997, 2984, 1012, 102], [101, 2043, 2106, 1996, 24105, 2932, 1997, 10289, 8214, 4088, 4640, 1029, 102, 2004, 2012, 2087, 2060, 5534, 1010, 10289, 8214, 1005, 1055, 2493, 2448, 1037, 2193, 1997, 2739, 2865, 11730, 1012, 1996, 3157, 3076, 1011, 2448, 11730, 2421, 2093, 6399, 1010, 2119, 1037, 2557, 1998, 2547, 2276, 1010, 1998, 2195, 7298, 1998, 9263, 1012, 5625, 2004, 1037, 2028, 1011, 3931, 3485, 1999, 2244, 7326, 1010, 1996, 24105, 2932, 2003, 3843, 3807, 7058, 1998, 4447, 2000, 2022, 1996, 4587, 7142, 9234, 4772, 1999, 1996, 2142, 2163, 1012, 1996, 2060, 2932, 1010, 1996, 26536, 17420, 1010, 2003, 2207, 3807, 1037, 102], [101, 2043, 2106, 1996, 24105, 2932, 1997, 10289, 8214, 4088, 4640, 1029, 102, 1998, 2195, 7298, 1998, 9263, 1012, 5625, 2004, 1037, 2028, 1011, 3931, 3485, 1999, 2244, 7326, 1010, 1996, 24105, 2932, 2003, 3843, 3807, 7058, 1998, 4447, 2000, 2022, 1996, 4587, 7142, 9234, 4772, 1999, 1996, 2142, 2163, 1012, 1996, 2060, 2932, 1010, 1996, 26536, 17420, 1010, 2003, 2207, 3807, 1037, 2095, 1998, 7679, 2006, 3076, 3906, 1998, 8266, 1012, 1996, 8514, 24803, 2003, 2405, 6604, 1012, 1996, 6399, 2031, 9671, 4772, 5426, 1010, 2007, 1996, 9718, 2405, 3679, 1998, 3701, 7316, 2118, 1998, 2060, 2739, 1010, 102], [101, 2043, 2106, 1996, 24105, 2932, 1997, 10289, 8214, 4088, 4640, 1029, 102, 2163, 1012, 1996, 2060, 2932, 1010, 1996, 26536, 17420, 1010, 2003, 2207, 3807, 1037, 2095, 1998, 7679, 2006, 3076, 3906, 1998, 8266, 1012, 1996, 8514, 24803, 2003, 2405, 6604, 1012, 1996, 6399, 2031, 9671, 4772, 5426, 1010, 2007, 1996, 9718, 2405, 3679, 1998, 3701, 7316, 2118, 1998, 2060, 2739, 1010, 1998, 21121, 2011, 2493, 2013, 2119, 10289, 8214, 1998, 3002, 2984, 1005, 1055, 2267, 1012, 4406, 24105, 1998, 1996, 8514, 1010, 1996, 9718, 2003, 2019, 2981, 4772, 1998, 2515, 2025, 2031, 1037, 4513, 8619, 2030, 2151, 102], [101, 2043, 2106, 1996, 24105, 2932, 1997, 10289, 8214, 4088, 4640, 1029, 102, 1010, 2007, 1996, 9718, 2405, 3679, 1998, 3701, 7316, 2118, 1998, 2060, 2739, 1010, 1998, 21121, 2011, 2493, 2013, 2119, 10289, 8214, 1998, 3002, 2984, 1005, 1055, 2267, 1012, 4406, 24105, 1998, 1996, 8514, 1010, 1996, 9718, 2003, 2019, 2981, 4772, 1998, 2515, 2025, 2031, 1037, 4513, 8619, 2030, 2151, 8368, 15709, 2013, 1996, 2118, 1012, 1999, 3055, 1010, 2043, 2070, 2493, 3373, 2008, 1996, 9718, 2211, 2000, 2265, 1037, 4603, 13827, 1010, 1037, 4314, 3780, 1010, 2691, 3168, 2001, 2405, 1012, 10655, 1010, 1999, 2494, 102], [101, 2043, 2106, 1996, 24105, 2932, 1997, 10289, 8214, 4088, 4640, 1029, 102, 9718, 2003, 2019, 2981, 4772, 1998, 2515, 2025, 2031, 1037, 4513, 8619, 2030, 2151, 8368, 15709, 2013, 1996, 2118, 1012, 1999, 3055, 1010, 2043, 2070, 2493, 3373, 2008, 1996, 9718, 2211, 2000, 2265, 1037, 4603, 13827, 1010, 1037, 4314, 3780, 1010, 2691, 3168, 2001, 2405, 1012, 10655, 1010, 1999, 2494, 1010, 2043, 2060, 2493, 3373, 2008, 1996, 3259, 3662, 1037, 4314, 13827, 1010, 1996, 4603, 3259, 3493, 13631, 2253, 2046, 2537, 1012, 4445, 3259, 2003, 2405, 2004, 2411, 2004, 1996, 9718, 1025, 2174, 1010, 2035, 2093, 102], [101, 2043, 2106, 1996, 24105, 2932, 1997, 10289, 8214, 4088, 4640, 1029, 102, 1010, 1037, 4314, 3780, 1010, 2691, 3168, 2001, 2405, 1012, 10655, 1010, 1999, 2494, 1010, 2043, 2060, 2493, 3373, 2008, 1996, 3259, 3662, 1037, 4314, 13827, 1010, 1996, 4603, 3259, 3493, 13631, 2253, 2046, 2537, 1012, 4445, 3259, 2003, 2405, 2004, 2411, 2004, 1996, 9718, 1025, 2174, 1010, 2035, 2093, 2024, 5500, 2000, 2035, 2493, 1012, 2633, 1010, 1999, 3500, 2263, 2019, 8324, 3485, 2005, 2576, 2671, 2470, 1010, 3458, 4331, 1010, 2081, 2049, 2834, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'offset_mapping': [[(0, 0), (0, 3), (4, 12), (13, 15), (16, 19), (20, 26), (27, 32), (33, 35), (36, 41), (42, 46), (47, 49), (50, 56), (57, 59), (60, 65), (66, 75), (75, 76), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 252), (252, 254), (254, 256), (257, 259), (260, 262), (263, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 365), (366, 368), (369, 372), (373, 375), (0, 0)], [(0, 0), (0, 3), (4, 12), (13, 15), (16, 19), (20, 26), (27, 32), (33, 35), (36, 41), (42, 46), (47, 49), (50, 56), (57, 59), (60, 65), (66, 75), (75, 76), (0, 0), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 252), (252, 254), (254, 256), (257, 259), (260, 262), (263, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 365), (366, 368), (369, 372), (373, 375), (375, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 447), (447, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (0, 0)], [(0, 0), (0, 3), (4, 12), (13, 15), (16, 19), (20, 26), (27, 32), (33, 35), (36, 41), (42, 46), (47, 49), (50, 56), (57, 59), (60, 65), (66, 75), (75, 76), (0, 0), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 365), (366, 368), (369, 372), (373, 375), (375, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 447), (447, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 536), (536, 539), (539, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (0, 0)], [(0, 0), (0, 3), (4, 12), (13, 15), (16, 19), (20, 26), (27, 32), (33, 35), (36, 41), (42, 46), (47, 49), (50, 56), (57, 59), (60, 65), (66, 75), (75, 76), (0, 0), (438, 440), (441, 444), (445, 447), (447, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 536), (536, 539), (539, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 11), (12, 14), (14, 18), (19, 21), (22, 27), (28, 32), (32, 33), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 252), (252, 254), (254, 256), (257, 259), (260, 262), (263, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 365), (366, 368), (369, 372), (373, 375), (375, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 11), (12, 14), (14, 18), (19, 21), (22, 27), (28, 32), (32, 33), (0, 0), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 252), (252, 254), (254, 256), (257, 259), (260, 262), (263, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 365), (366, 368), (369, 372), (373, 375), (375, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 447), (447, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 536), (536, 539), (539, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (0, 0)], [(0, 0), (0, 4), (5, 7), (8, 11), (12, 14), (14, 18), (19, 21), (22, 27), (28, 32), (32, 33), (0, 0), (346, 352), (353, 356), (357, 365), (366, 368), (369, 372), (373, 375), (375, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 447), (447, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 536), (536, 539), (539, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0)], [(0, 0), (0, 4), (5, 9), (10, 12), (13, 16), (17, 19), (20, 23), (24, 28), (29, 37), (38, 40), (41, 46), (47, 51), (51, 52), (0, 0), (0, 13), (13, 15), (15, 16), (17, 20), (21, 27), (28, 31), (32, 33), (34, 42), (43, 52), (52, 53), (54, 58), (59, 62), (63, 67), (68, 76), (76, 77), (77, 78), (79, 83), (84, 88), (89, 91), (92, 93), (94, 100), (101, 107), (108, 110), (111, 114), (115, 121), (122, 126), (126, 127), (128, 139), (140, 142), (143, 148), (149, 151), (152, 155), (156, 160), (161, 169), (170, 173), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 252), (252, 254), (254, 256), (257, 259), (260, 262), (263, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 365), (366, 368), (369, 372), (373, 375), (375, 379), (379, 380), (381, 382), (0, 0)], [(0, 0), (0, 4), (5, 9), (10, 12), (13, 16), (17, 19), (20, 23), (24, 28), (29, 37), (38, 40), (41, 46), (47, 51), (51, 52), (0, 0), (174, 180), (181, 183), (183, 184), (185, 187), (188, 189), (190, 196), (197, 203), (204, 206), (207, 213), (214, 218), (219, 223), (224, 226), (226, 229), (229, 232), (233, 237), (238, 241), (242, 248), (249, 250), (250, 252), (252, 254), (254, 256), (257, 259), (260, 262), (263, 265), (265, 268), (268, 269), (269, 270), (271, 275), (276, 278), (279, 282), (283, 287), (288, 296), (297, 299), (300, 303), (304, 312), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 365), (366, 368), (369, 372), (373, 375), (375, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 447), (447, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 536), (536, 539), (0, 0)], [(0, 0), (0, 4), (5, 9), (10, 12), (13, 16), (17, 19), (20, 23), (24, 28), (29, 37), (38, 40), (41, 46), (47, 51), (51, 52), (0, 0), (313, 315), (316, 319), (320, 326), (327, 332), (332, 333), (334, 345), (346, 352), (353, 356), (357, 365), (366, 368), (369, 372), (373, 375), (375, 379), (379, 380), (381, 382), (383, 389), (390, 395), (396, 398), (399, 405), (406, 409), (410, 420), (420, 421), (422, 424), (425, 427), (428, 429), (430, 437), (438, 440), (441, 444), (445, 447), (447, 451), (452, 454), (455, 458), (458, 462), (462, 463), (464, 470), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 536), (536, 539), (539, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (0, 0)], [(0, 0), (0, 4), (5, 9), (10, 12), (13, 16), (17, 19), (20, 23), (24, 28), (29, 37), (38, 40), (41, 46), (47, 51), (51, 52), (0, 0), (471, 476), (477, 480), (481, 487), (488, 492), (493, 500), (500, 502), (503, 511), (512, 514), (515, 520), (521, 525), (525, 528), (528, 531), (532, 534), (534, 536), (536, 539), (539, 541), (542, 544), (545, 549), (549, 550), (551, 553), (554, 557), (558, 561), (562, 564), (565, 568), (569, 573), (574, 579), (580, 581), (581, 584), (585, 587), (588, 589), (590, 596), (597, 601), (602, 606), (607, 615), (616, 623), (624, 625), (626, 633), (634, 637), (638, 641), (642, 646), (647, 651), (651, 652), (652, 653), (654, 656), (657, 658), (659, 665), (665, 666), (667, 673), (674, 679), (680, 686), (687, 689), (690, 694), (694, 695), (0, 0)], [(0, 0), (0, 4), (5, 8), (9, 12), (13, 23), (24, 32), (33, 35), (36, 41), (42, 46), (47, 52), (53, 63), (63, 64), (0, 0), (0, 2), (3, 5), (6, 10), (11, 16), (17, 29), (29, 30), (31, 36), (37, 41), (41, 42), (42, 43), (44, 52), (53, 56), (57, 58), (59, 65), (66, 68), (69, 73), (74, 79), (80, 87), (87, 88), (89, 92), (93, 97), (98, 105), (105, 106), (106, 109), (110, 117), (118, 125), (126, 131), (132, 142), (142, 143), (144, 148), (149, 150), (151, 156), (157, 160), (161, 171), (172, 179), (179, 180), (181, 184), (185, 192), (193, 202), (203, 206), (207, 215), (215, 216), (217, 222), (223, 225), (226, 227), (228, 231), (231, 232), (232, 236), (237, 244), (245, 247), (248, 257), (258, 262), (262, 263), (264, 267), (268, 278), (279, 287), (288, 290), (291, 297), (298, 303), (304, 311), (312, 315), (316, 322), (323, 325), (326, 328), (329, 332), (333, 339), (340, 350), (351, 361), (362, 373), (374, 376), (377, 380), (381, 387), (388, 394), (394, 395), (396, 399), (400, 405), (406, 414), (414, 415), (416, 419), (420, 423), (423, 427), (427, 428), (429, 431), (432, 440), (441, 446), (447, 448), (0, 0)], [(0, 0), (0, 4), (5, 8), (9, 12), (13, 23), (24, 32), (33, 35), (36, 41), (42, 46), (47, 52), (53, 63), (63, 64), (0, 0), (181, 184), (185, 192), (193, 202), (203, 206), (207, 215), (215, 216), (217, 222), (223, 225), (226, 227), (228, 231), (231, 232), (232, 236), (237, 244), (245, 247), (248, 257), (258, 262), (262, 263), (264, 267), (268, 278), (279, 287), (288, 290), (291, 297), (298, 303), (304, 311), (312, 315), (316, 322), (323, 325), (326, 328), (329, 332), (333, 339), (340, 350), (351, 361), (362, 373), (374, 376), (377, 380), (381, 387), (388, 394), (394, 395), (396, 399), (400, 405), (406, 414), (414, 415), (416, 419), (420, 423), (423, 427), (427, 428), (429, 431), (432, 440), (441, 446), (447, 448), (449, 453), (454, 457), (458, 465), (466, 468), (469, 476), (477, 487), (488, 491), (492, 499), (499, 500), (501, 504), (505, 509), (510, 518), (519, 521), (522, 531), (532, 540), (540, 541), (542, 545), (546, 556), (557, 561), (562, 569), (570, 581), (582, 591), (591, 592), (593, 597), (598, 601), (602, 610), (611, 620), (621, 626), (627, 630), (631, 637), (638, 647), (648, 658), (659, 662), (663, 668), (669, 673), (673, 674), (0, 0)], [(0, 0), (0, 4), (5, 8), (9, 12), (13, 23), (24, 32), (33, 35), (36, 41), (42, 46), (47, 52), (53, 63), (63, 64), (0, 0), (388, 394), (394, 395), (396, 399), (400, 405), (406, 414), (414, 415), (416, 419), (420, 423), (423, 427), (427, 428), (429, 431), (432, 440), (441, 446), (447, 448), (449, 453), (454, 457), (458, 465), (466, 468), (469, 476), (477, 487), (488, 491), (492, 499), (499, 500), (501, 504), (505, 509), (510, 518), (519, 521), (522, 531), (532, 540), (540, 541), (542, 545), (546, 556), (557, 561), (562, 569), (570, 581), (582, 591), (591, 592), (593, 597), (598, 601), (602, 610), (611, 620), (621, 626), (627, 630), (631, 637), (638, 647), (648, 658), (659, 662), (663, 668), (669, 673), (673, 674), (675, 678), (679, 686), (687, 689), (690, 698), (699, 703), (704, 708), (709, 714), (715, 719), (720, 723), (724, 729), (730, 734), (734, 735), (735, 736), (737, 744), (744, 745), (746, 752), (753, 763), (764, 767), (768, 771), (772, 776), (776, 777), (778, 781), (782, 790), (791, 793), (794, 796), (797, 808), (809, 820), (821, 824), (825, 829), (830, 833), (834, 838), (839, 840), (841, 848), (849, 856), (857, 859), (860, 863), (0, 0)], [(0, 0), (0, 4), (5, 8), (9, 12), (13, 23), (24, 32), (33, 35), (36, 41), (42, 46), (47, 52), (53, 63), (63, 64), (0, 0), (591, 592), (593, 597), (598, 601), (602, 610), (611, 620), (621, 626), (627, 630), (631, 637), (638, 647), (648, 658), (659, 662), (663, 668), (669, 673), (673, 674), (675, 678), (679, 686), (687, 689), (690, 698), (699, 703), (704, 708), (709, 714), (715, 719), (720, 723), (724, 729), (730, 734), (734, 735), (735, 736), (737, 744), (744, 745), (746, 752), (753, 763), (764, 767), (768, 771), (772, 776), (776, 777), (778, 781), (782, 790), (791, 793), (794, 796), (797, 808), (809, 820), (821, 824), (825, 829), (830, 833), (834, 838), (839, 840), (841, 848), (849, 856), (857, 859), (860, 863), (864, 873), (874, 883), (884, 888), (889, 892), (893, 903), (903, 904), (905, 907), (908, 912), (912, 913), (914, 918), (919, 923), (924, 932), (933, 941), (942, 946), (947, 950), (951, 959), (960, 965), (966, 968), (969, 973), (974, 975), (976, 988), (989, 993), (993, 994), (995, 996), (997, 1004), (1005, 1014), (1014, 1015), (1016, 1022), (1023, 1028), (1029, 1032), (1033, 1042), (1042, 1043), (1044, 1052), (1052, 1053), (1054, 1056), (1057, 1061), (0, 0)], [(0, 0), (0, 4), (5, 8), (9, 12), (13, 23), (24, 32), (33, 35), (36, 41), (42, 46), (47, 52), (53, 63), (63, 64), (0, 0), (782, 790), (791, 793), (794, 796), (797, 808), (809, 820), (821, 824), (825, 829), (830, 833), (834, 838), (839, 840), (841, 848), (849, 856), (857, 859), (860, 863), (864, 873), (874, 883), (884, 888), (889, 892), (893, 903), (903, 904), (905, 907), (908, 912), (912, 913), (914, 918), (919, 923), (924, 932), (933, 941), (942, 946), (947, 950), (951, 959), (960, 965), (966, 968), (969, 973), (974, 975), (976, 988), (989, 993), (993, 994), (995, 996), (997, 1004), (1005, 1014), (1014, 1015), (1016, 1022), (1023, 1028), (1029, 1032), (1033, 1042), (1042, 1043), (1044, 1052), (1052, 1053), (1054, 1056), (1057, 1061), (1061, 1062), (1063, 1067), (1068, 1073), (1074, 1082), (1083, 1091), (1092, 1096), (1097, 1100), (1101, 1106), (1107, 1113), (1114, 1115), (1116, 1123), (1124, 1128), (1128, 1129), (1130, 1133), (1134, 1146), (1147, 1152), (1153, 1158), (1159, 1164), (1165, 1169), (1170, 1174), (1175, 1185), (1185, 1186), (1187, 1194), (1195, 1200), (1201, 1203), (1204, 1213), (1214, 1216), (1217, 1222), (1223, 1225), (1226, 1229), (1230, 1238), (1238, 1239), (1240, 1247), (1247, 1248), (1249, 1252), (1253, 1258), (0, 0)], [(0, 0), (0, 4), (5, 8), (9, 12), (13, 23), (24, 32), (33, 35), (36, 41), (42, 46), (47, 52), (53, 63), (63, 64), (0, 0), (993, 994), (995, 996), (997, 1004), (1005, 1014), (1014, 1015), (1016, 1022), (1023, 1028), (1029, 1032), (1033, 1042), (1042, 1043), (1044, 1052), (1052, 1053), (1054, 1056), (1057, 1061), (1061, 1062), (1063, 1067), (1068, 1073), (1074, 1082), (1083, 1091), (1092, 1096), (1097, 1100), (1101, 1106), (1107, 1113), (1114, 1115), (1116, 1123), (1124, 1128), (1128, 1129), (1130, 1133), (1134, 1146), (1147, 1152), (1153, 1158), (1159, 1164), (1165, 1169), (1170, 1174), (1175, 1185), (1185, 1186), (1187, 1194), (1195, 1200), (1201, 1203), (1204, 1213), (1214, 1216), (1217, 1222), (1223, 1225), (1226, 1229), (1230, 1238), (1238, 1239), (1240, 1247), (1247, 1248), (1249, 1252), (1253, 1258), (1259, 1262), (1263, 1274), (1275, 1277), (1278, 1281), (1282, 1290), (1290, 1291), (1292, 1299), (1299, 1300), (1301, 1303), (1304, 1310), (1311, 1315), (1316, 1318), (1319, 1332), (1333, 1340), (1341, 1344), (1345, 1354), (1355, 1362), (1363, 1371), (1371, 1372), (1373, 1379), (1380, 1388), (1388, 1389), (1390, 1394), (1395, 1398), (1399, 1404), (1404, 1405), (0, 0)]], 'overflow_to_sample_mapping': [0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3]},\n dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping']),\n 100)"},"metadata":{}}]},{"cell_type":"code","source":"for k,v in sample_encoding.items():\n  print(f\"shape of {k} :: {len(v)}\")  # 4 inputs  results in 19 samples","metadata":{"id":"cUHLHqZInL_Z","outputId":"406073e9-2286-42be-be71-7c5ec36808c8","execution":{"iopub.status.busy":"2022-05-29T11:30:56.553482Z","iopub.execute_input":"2022-05-29T11:30:56.554074Z","iopub.status.idle":"2022-05-29T11:30:56.560684Z","shell.execute_reply.started":"2022-05-29T11:30:56.554034Z","shell.execute_reply":"2022-05-29T11:30:56.559725Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"shape of input_ids :: 17\nshape of token_type_ids :: 17\nshape of attention_mask :: 17\nshape of offset_mapping :: 17\nshape of overflow_to_sample_mapping :: 17\n","output_type":"stream"}]},{"cell_type":"markdown","source":"input_ids ,token_type_ids,attention_mask,offset_mapping : each of them will be list of lists and overflow_to_sample_mapping will be simple list","metadata":{"id":"1lniCdkWGAGH"}},{"cell_type":"markdown","source":"let's make the labels. labels will be start_positions and end_positions where each of them will be of shape (batch_size)","metadata":{"id":"01Lpv5pVGiEZ"}},{"cell_type":"markdown","source":"(0, 0) if the answer is not in the corresponding span of the context\n(start_position, end_position) if the answer is in the corresponding span of the context, with start_position being the index of the token (in the input IDs) at the start of the answer and end_position being the index of the token (in the input IDs) where the answer ends","metadata":{"id":"Cz_Hiu5fpI64"}},{"cell_type":"code","source":"sample_answers = raw_datasets[\"train\"][2:6][\"answers\"]\nsample_answers,sample_answers[0]","metadata":{"id":"A00Datp9pLAx","outputId":"e4afa8bf-bf9d-4574-a470-0d9fceb8fc72","execution":{"iopub.status.busy":"2022-05-29T11:30:56.562624Z","iopub.execute_input":"2022-05-29T11:30:56.563293Z","iopub.status.idle":"2022-05-29T11:30:56.573575Z","shell.execute_reply.started":"2022-05-29T11:30:56.563255Z","shell.execute_reply":"2022-05-29T11:30:56.572381Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"([{'text': ['the Main Building'], 'answer_start': [279]},\n  {'text': ['a Marian place of prayer and reflection'], 'answer_start': [381]},\n  {'text': ['a golden statue of the Virgin Mary'], 'answer_start': [92]},\n  {'text': ['September 1876'], 'answer_start': [248]}],\n {'text': ['the Main Building'], 'answer_start': [279]})"},"metadata":{}}]},{"cell_type":"code","source":"sample_encoding['overflow_to_sample_mapping']\n","metadata":{"id":"EMM2UPpbrFJ5","outputId":"763c3589-b7b6-4abe-cbe8-335beb6c7d00","execution":{"iopub.status.busy":"2022-05-29T11:30:56.574693Z","iopub.execute_input":"2022-05-29T11:30:56.575566Z","iopub.status.idle":"2022-05-29T11:30:56.584074Z","shell.execute_reply.started":"2022-05-29T11:30:56.575537Z","shell.execute_reply":"2022-05-29T11:30:56.583072Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"[0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3]"},"metadata":{}}]},{"cell_type":"code","source":"# find the original sample\n# find answers start and end char positions of that original sample\n# Find the start and end of the context\n# If the answer is not fully inside the context, label is (0, 0)\n# Otherwise it's the start and end token positions\nsample_mappings = sample_encoding['overflow_to_sample_mapping']\nstart_positions = []\nend_positions = []\nfor i,offset in enumerate(sample_encoding['offset_mapping']):\n  original_sample_id = sample_mappings[i] #find the original sample\n  answer = sample_answers[original_sample_id]\n  answer_start = answer['answer_start'][0]\n  answer_end = answer_start+len(answer['text'][0])\n  sequence_id = sample_encoding.sequence_ids(i)\n  idx = 0\n  while sequence_id[idx]!=1:\n    idx +=1\n  context_start = idx\n  while sequence_id[idx]==1:\n    idx +=1\n  context_end = idx-1\n  if offset[context_start][0]>answer_start or offset[context_end][1]<answer_end:\n    start_positions.append(0)\n    end_positions.append(0)\n  else:\n    idx = context_start\n    while idx <= context_end and offset[idx][0] <= answer_start:\n      idx +=1\n    start_positions.append(idx-1)\n    idx = context_end\n    while idx >= context_start and offset[idx][1] >= answer_end:\n      idx -= 1\n    end_positions.append(idx+1)\nstart_positions, end_positions\n\n\n","metadata":{"id":"Mwr4808TNkD5","outputId":"c4e09c83-2df4-4ad0-f493-4b483887e8b0","execution":{"iopub.status.busy":"2022-05-29T11:30:56.586163Z","iopub.execute_input":"2022-05-29T11:30:56.586909Z","iopub.status.idle":"2022-05-29T11:30:56.604229Z","shell.execute_reply.started":"2022-05-29T11:30:56.586834Z","shell.execute_reply":"2022-05-29T11:30:56.603362Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"([81, 49, 17, 0, 0, 57, 19, 33, 0, 0, 0, 63, 27, 0, 0, 0, 0],\n [83, 51, 19, 0, 0, 63, 25, 39, 0, 0, 0, 64, 28, 0, 0, 0, 0])"},"metadata":{}}]},{"cell_type":"markdown","source":"Let’s take a look at a few results to verify that our approach is correct. For the first feature we find (83, 85) as labels, so let’s compare the theoretical answer with the decoded span of tokens from 83 to 85 (inclusive):","metadata":{"id":"hqh_JX2ygMdP"}},{"cell_type":"code","source":"idx = 0\nsample_idx = sample_encoding[\"overflow_to_sample_mapping\"][idx]\nanswer = sample_answers[sample_idx][\"text\"][0]\n\nstart = start_positions[idx]\nend = end_positions[idx]\nlabeled_answer = tokenizer.decode(sample_encoding[\"input_ids\"][idx][start : end + 1])\n\nprint(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")","metadata":{"id":"fSIHC2WLf09v","outputId":"7f60c782-e4e6-4221-cc4d-116c24bc4249","execution":{"iopub.status.busy":"2022-05-29T11:30:56.607559Z","iopub.execute_input":"2022-05-29T11:30:56.608361Z","iopub.status.idle":"2022-05-29T11:30:56.617551Z","shell.execute_reply.started":"2022-05-29T11:30:56.608317Z","shell.execute_reply":"2022-05-29T11:30:56.61668Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Theoretical answer: the Main Building, labels give: the main building\n","output_type":"stream"}]},{"cell_type":"code","source":"idx = 4\nsample_idx = sample_encoding[\"overflow_to_sample_mapping\"][idx]\nanswer = sample_answers[sample_idx][\"text\"][0]\n\ndecoded_example = tokenizer.decode(sample_encoding[\"input_ids\"][idx])\nprint(f\"Theoretical answer: {answer}, decoded example: {decoded_example}\") #we don’t see the answer inside the context.","metadata":{"id":"TVH1fHz8geo2","outputId":"44cb4ae3-93bc-4b7e-8736-ae7ad6f75162","execution":{"iopub.status.busy":"2022-05-29T11:30:56.619068Z","iopub.execute_input":"2022-05-29T11:30:56.619762Z","iopub.status.idle":"2022-05-29T11:30:56.629393Z","shell.execute_reply.started":"2022-05-29T11:30:56.61972Z","shell.execute_reply":"2022-05-29T11:30:56.628386Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Theoretical answer: a Marian place of prayer and reflection, decoded example: [CLS] what is the grotto at notre dame? [SEP] architecturally, the school has a catholic character. atop the main building's gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of [SEP]\n","output_type":"stream"}]},{"cell_type":"code","source":"max_length = 384\nstride = 128\n\n\ndef preprocess_training_examples(examples):\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    offset_mapping = inputs.pop(\"offset_mapping\")\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    answers = examples[\"answers\"]\n    start_positions = []\n    end_positions = []\n\n    for i, offset in enumerate(offset_mapping):\n        sample_idx = sample_map[i]\n        answer = answers[sample_idx]\n        start_char = answer[\"answer_start\"][0]\n        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n        sequence_ids = inputs.sequence_ids(i)\n\n        # Find the start and end of the context\n        idx = 0\n        while sequence_ids[idx] != 1:\n            idx += 1\n        context_start = idx\n        while sequence_ids[idx] == 1:\n            idx += 1\n        context_end = idx - 1\n\n        # If the answer is not fully inside the context, label is (0, 0)\n        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            # Otherwise it's the start and end token positions\n            idx = context_start\n            while idx <= context_end and offset[idx][0] <= start_char:\n                idx += 1\n            start_positions.append(idx - 1)\n\n            idx = context_end\n            while idx >= context_start and offset[idx][1] >= end_char:\n                idx -= 1\n            end_positions.append(idx + 1)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs","metadata":{"id":"J6gbBlt2P_ef","execution":{"iopub.status.busy":"2022-05-29T11:30:56.63091Z","iopub.execute_input":"2022-05-29T11:30:56.631487Z","iopub.status.idle":"2022-05-29T11:30:56.648218Z","shell.execute_reply.started":"2022-05-29T11:30:56.63145Z","shell.execute_reply":"2022-05-29T11:30:56.647224Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset = raw_datasets.map(\n    preprocess_training_examples,\n    batched=True,\n    remove_columns=raw_datasets[\"train\"].column_names,\n)","metadata":{"id":"BI9g_21Yi-UF","outputId":"206bb5f1-0e47-4c5d-d123-1af4f45f581f","execution":{"iopub.status.busy":"2022-05-29T11:30:56.65211Z","iopub.execute_input":"2022-05-29T11:30:56.653616Z","iopub.status.idle":"2022-05-29T11:32:30.578039Z","shell.execute_reply.started":"2022-05-29T11:30:56.65357Z","shell.execute_reply":"2022-05-29T11:32:30.577166Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/88 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16d75346b59f426182821dbcc177ad4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/11 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"481c0d55b99c42c7b555c24148d9e14e"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_dataset","metadata":{"id":"qRSx5u0kovqK","outputId":"a36d1375-9f78-415d-a27e-d275f3e67834","execution":{"iopub.status.busy":"2022-05-29T11:32:30.580125Z","iopub.execute_input":"2022-05-29T11:32:30.580658Z","iopub.status.idle":"2022-05-29T11:32:30.588082Z","shell.execute_reply.started":"2022-05-29T11:32:30.58062Z","shell.execute_reply":"2022-05-29T11:32:30.587402Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n        num_rows: 88524\n    })\n    validation: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n        num_rows: 10784\n    })\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Fine tuning the model","metadata":{"id":"rbZKpw3QlYOc"}},{"cell_type":"code","source":"from transformers import AutoModelForQuestionAnswering\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)","metadata":{"id":"PRXfJP5TlFZV","outputId":"a5ccc05f-e531-47be-bb46-070d9cdbd44b","execution":{"iopub.status.busy":"2022-05-29T11:32:30.589384Z","iopub.execute_input":"2022-05-29T11:32:30.589882Z","iopub.status.idle":"2022-05-29T11:32:50.598122Z","shell.execute_reply.started":"2022-05-29T11:32:30.589843Z","shell.execute_reply":"2022-05-29T11:32:50.597227Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f930eb65eeaa41478ccb5b79df5d948d"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"id":"uRzgvx9JlxC7","outputId":"8488d4ea-a01a-4385-d07f-bdd8e9aca6e7","execution":{"iopub.status.busy":"2022-05-29T11:35:45.562072Z","iopub.execute_input":"2022-05-29T11:35:45.562485Z","iopub.status.idle":"2022-05-29T11:35:45.655739Z","shell.execute_reply.started":"2022-05-29T11:35:45.562451Z","shell.execute_reply":"2022-05-29T11:35:45.654885Z"},"trusted":true},"execution_count":44,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fbecb7b8f90478eb895cc1712e57386"}},"metadata":{}}]},{"cell_type":"code","source":"!apt install git-lfs","metadata":{"id":"zXdRB9fTnvhi","outputId":"84fbc36c-5a5c-44c5-9552-2ec23f7e8704","execution":{"iopub.status.busy":"2022-05-29T11:34:31.096148Z","iopub.execute_input":"2022-05-29T11:34:31.097325Z","iopub.status.idle":"2022-05-29T11:34:33.276152Z","shell.execute_reply.started":"2022-05-29T11:34:31.097252Z","shell.execute_reply":"2022-05-29T11:34:33.275107Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\ngit-lfs is already the newest version (2.9.2-1).\n0 upgraded, 0 newly installed, 0 to remove and 27 not upgraded.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    \"bert-finetuned-squad\",\n    evaluation_strategy=\"no\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    fp16=True,\n    push_to_hub=True,\n)","metadata":{"id":"4GeGeRDwnOAF","execution":{"iopub.status.busy":"2022-05-29T11:36:04.776911Z","iopub.execute_input":"2022-05-29T11:36:04.777442Z","iopub.status.idle":"2022-05-29T11:36:04.789796Z","shell.execute_reply.started":"2022-05-29T11:36:04.777398Z","shell.execute_reply":"2022-05-29T11:36:04.788843Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2022-05-29T11:36:08.085356Z","iopub.execute_input":"2022-05-29T11:36:08.08602Z","iopub.status.idle":"2022-05-29T11:36:08.090176Z","shell.execute_reply.started":"2022-05-29T11:36:08.085977Z","shell.execute_reply":"2022-05-29T11:36:08.089322Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized_dataset['train'],\n    eval_dataset=tokenized_dataset['validation'],\n    tokenizer=tokenizer,\n)\ntrainer.train()","metadata":{"id":"OyhJ8giuncnS","outputId":"457d5334-2283-474a-839f-ecc3440ac380","execution":{"iopub.status.busy":"2022-05-29T11:36:10.601397Z","iopub.execute_input":"2022-05-29T11:36:10.602372Z","iopub.status.idle":"2022-05-29T14:55:10.76791Z","shell.execute_reply.started":"2022-05-29T11:36:10.602309Z","shell.execute_reply":"2022-05-29T14:55:10.766872Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/working/bert-finetuned-squad is already a clone of https://huggingface.co/susghosh/bert-finetuned-squad. Make sure you pull the latest changes with `repo.git_pull()`.\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"Using amp half precision backend\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 88524\n  Num Epochs = 3\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 33198\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='33198' max='33198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [33198/33198 3:18:55, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>2.835600</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.673200</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.499200</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.399400</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.367800</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.317900</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.263100</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>1.210100</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>1.197600</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>1.201600</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>1.159700</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>1.119600</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>1.097600</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>1.045100</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>1.131200</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>1.130900</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>1.098500</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>1.068600</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>1.013300</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>1.071000</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>1.055600</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>1.040400</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.769100</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.776500</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.758500</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.767100</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.788300</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.708100</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.767700</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.754900</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>0.762600</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.771100</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>0.775200</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>0.763600</td>\n    </tr>\n    <tr>\n      <td>17500</td>\n      <td>0.750200</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>0.767200</td>\n    </tr>\n    <tr>\n      <td>18500</td>\n      <td>0.744200</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>0.736200</td>\n    </tr>\n    <tr>\n      <td>19500</td>\n      <td>0.716500</td>\n    </tr>\n    <tr>\n      <td>20000</td>\n      <td>0.716200</td>\n    </tr>\n    <tr>\n      <td>20500</td>\n      <td>0.778600</td>\n    </tr>\n    <tr>\n      <td>21000</td>\n      <td>0.774500</td>\n    </tr>\n    <tr>\n      <td>21500</td>\n      <td>0.751800</td>\n    </tr>\n    <tr>\n      <td>22000</td>\n      <td>0.762600</td>\n    </tr>\n    <tr>\n      <td>22500</td>\n      <td>0.596100</td>\n    </tr>\n    <tr>\n      <td>23000</td>\n      <td>0.513300</td>\n    </tr>\n    <tr>\n      <td>23500</td>\n      <td>0.519300</td>\n    </tr>\n    <tr>\n      <td>24000</td>\n      <td>0.503400</td>\n    </tr>\n    <tr>\n      <td>24500</td>\n      <td>0.525800</td>\n    </tr>\n    <tr>\n      <td>25000</td>\n      <td>0.495700</td>\n    </tr>\n    <tr>\n      <td>25500</td>\n      <td>0.527700</td>\n    </tr>\n    <tr>\n      <td>26000</td>\n      <td>0.505900</td>\n    </tr>\n    <tr>\n      <td>26500</td>\n      <td>0.514300</td>\n    </tr>\n    <tr>\n      <td>27000</td>\n      <td>0.520100</td>\n    </tr>\n    <tr>\n      <td>27500</td>\n      <td>0.517500</td>\n    </tr>\n    <tr>\n      <td>28000</td>\n      <td>0.522500</td>\n    </tr>\n    <tr>\n      <td>28500</td>\n      <td>0.520900</td>\n    </tr>\n    <tr>\n      <td>29000</td>\n      <td>0.531700</td>\n    </tr>\n    <tr>\n      <td>29500</td>\n      <td>0.502400</td>\n    </tr>\n    <tr>\n      <td>30000</td>\n      <td>0.519500</td>\n    </tr>\n    <tr>\n      <td>30500</td>\n      <td>0.523100</td>\n    </tr>\n    <tr>\n      <td>31000</td>\n      <td>0.513100</td>\n    </tr>\n    <tr>\n      <td>31500</td>\n      <td>0.511500</td>\n    </tr>\n    <tr>\n      <td>32000</td>\n      <td>0.524300</td>\n    </tr>\n    <tr>\n      <td>32500</td>\n      <td>0.503200</td>\n    </tr>\n    <tr>\n      <td>33000</td>\n      <td>0.493700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to bert-finetuned-squad/checkpoint-11066\nConfiguration saved in bert-finetuned-squad/checkpoint-11066/config.json\nModel weights saved in bert-finetuned-squad/checkpoint-11066/pytorch_model.bin\ntokenizer config file saved in bert-finetuned-squad/checkpoint-11066/tokenizer_config.json\nSpecial tokens file saved in bert-finetuned-squad/checkpoint-11066/special_tokens_map.json\ntokenizer config file saved in bert-finetuned-squad/tokenizer_config.json\nSpecial tokens file saved in bert-finetuned-squad/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to bert-finetuned-squad/checkpoint-22132\nConfiguration saved in bert-finetuned-squad/checkpoint-22132/config.json\nModel weights saved in bert-finetuned-squad/checkpoint-22132/pytorch_model.bin\ntokenizer config file saved in bert-finetuned-squad/checkpoint-22132/tokenizer_config.json\nSpecial tokens file saved in bert-finetuned-squad/checkpoint-22132/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"tokenizer config file saved in bert-finetuned-squad/tokenizer_config.json\nSpecial tokens file saved in bert-finetuned-squad/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to bert-finetuned-squad/checkpoint-33198\nConfiguration saved in bert-finetuned-squad/checkpoint-33198/config.json\nModel weights saved in bert-finetuned-squad/checkpoint-33198/pytorch_model.bin\ntokenizer config file saved in bert-finetuned-squad/checkpoint-33198/tokenizer_config.json\nSpecial tokens file saved in bert-finetuned-squad/checkpoint-33198/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"tokenizer config file saved in bert-finetuned-squad/tokenizer_config.json\nSpecial tokens file saved in bert-finetuned-squad/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=33198, training_loss=0.8475532442110476, metrics={'train_runtime': 11936.3434, 'train_samples_per_second': 22.249, 'train_steps_per_second': 2.781, 'total_flos': 5.204482670991974e+16, 'train_loss': 0.8475532442110476, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"PreProcessing the validation data","metadata":{"id":"dbkUZNfJjzj8"}},{"cell_type":"code","source":"# add one example_id column which will hel to identify from which original example the feature came\n# convert all the offset_mappings for question to None\n\ndef preprocess_eval_data(eval_data):\n  questions = [q.strip() for q in eval_data['question']]\n  batch_encoding = tokenizer(\n        questions,\n        eval_data[\"context\"],\n        max_length=384,\n        truncation=\"only_second\",\n        stride=128,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n  sample_map = batch_encoding.pop(\"overflow_to_sample_mapping\")\n  print(f\"len of sample_map {len(sample_map)} and len of input_ids {len(batch_encoding['input_ids'])}\")\n  example_ids = []\n  for i in range(len(batch_encoding['input_ids'])):\n    #for each question and cheunked context\n    sample_id = sample_map[i]\n    example_ids.append(eval_data['id'][sample_id])\n    sequence_ids =  batch_encoding.sequence_ids(i)\n    offset = batch_encoding['offset_mapping'][i]\n    batch_encoding['offset_mapping'][i] = [o if sequence_ids[k]==1 else None for k,o in enumerate(offset) ]\n  batch_encoding[\"example_id\"] = example_ids\n  return batch_encoding\n","metadata":{"id":"HVk5NfpmU5ge","execution":{"iopub.status.busy":"2022-05-29T11:33:04.423396Z","iopub.status.idle":"2022-05-29T11:33:04.424643Z","shell.execute_reply.started":"2022-05-29T11:33:04.424372Z","shell.execute_reply":"2022-05-29T11:33:04.424404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"small_eval_set = raw_datasets[\"validation\"].select(range(100))\ntrained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n\ntokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\neval_set = small_eval_set.map(\n    preprocess_eval_data,\n    batched=True,\n    remove_columns=raw_datasets[\"validation\"].column_names,\n)","metadata":{"id":"e_tWtzqralHG","outputId":"ccedacdc-696b-4b60-b172-ffb045af4647","execution":{"iopub.status.busy":"2022-05-29T11:33:04.426451Z","iopub.status.idle":"2022-05-29T11:33:04.42697Z","shell.execute_reply.started":"2022-05-29T11:33:04.426712Z","shell.execute_reply":"2022-05-29T11:33:04.426738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_set","metadata":{"id":"JA5OjfuVawp1","outputId":"220a1c66-900c-44b7-b3fe-6fbe2c32aed9","execution":{"iopub.status.busy":"2022-05-29T11:33:04.428709Z","iopub.status.idle":"2022-05-29T11:33:04.429232Z","shell.execute_reply.started":"2022-05-29T11:33:04.428967Z","shell.execute_reply":"2022-05-29T11:33:04.428995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_set['example_id']","metadata":{"id":"4PMqhqqPma70","outputId":"1a940ca1-5ed6-41e5-8255-17b3d5f43c6a","execution":{"iopub.status.busy":"2022-05-29T11:33:04.430824Z","iopub.status.idle":"2022-05-29T11:33:04.431376Z","shell.execute_reply.started":"2022-05-29T11:33:04.43109Z","shell.execute_reply":"2022-05-29T11:33:04.431116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"2ZNPB4XibL39","outputId":"0dba7771-d353-44ba-c76b-5f6d7d03c746","execution":{"iopub.status.busy":"2022-05-29T11:33:04.432923Z","iopub.status.idle":"2022-05-29T11:33:04.433467Z","shell.execute_reply.started":"2022-05-29T11:33:04.43318Z","shell.execute_reply":"2022-05-29T11:33:04.433207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"id":"26ZOPqP-bRwO","execution":{"iopub.status.busy":"2022-05-29T11:33:04.43504Z","iopub.status.idle":"2022-05-29T11:33:04.435987Z","shell.execute_reply.started":"2022-05-29T11:33:04.435725Z","shell.execute_reply":"2022-05-29T11:33:04.435752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForQuestionAnswering\neval_set_for_model = eval_set.remove_columns(['offset_mapping', 'example_id'])\neval_set_for_model.set_format(\"torch\")\ndevice = torch.device(\"cuda\") if torch.cuda.is_available else torch.device(\"cpu\")\nbatch = {k : eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\ntrained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(device)\nwith torch.no_grad():\n  outputs = trained_model(**batch)","metadata":{"id":"BazKNK21bjQ9","outputId":"38bfb131-0a6e-48d2-ba2f-44ec661591dd","execution":{"iopub.status.busy":"2022-05-29T11:33:04.437608Z","iopub.status.idle":"2022-05-29T11:33:04.43812Z","shell.execute_reply.started":"2022-05-29T11:33:04.437865Z","shell.execute_reply":"2022-05-29T11:33:04.437891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_set_for_model.column_names","metadata":{"id":"mO5bqXGsfIu0","outputId":"f4381d4d-6d13-4d80-bb4c-cc32b65c24c5","execution":{"iopub.status.busy":"2022-05-29T11:33:04.439595Z","iopub.status.idle":"2022-05-29T11:33:04.440084Z","shell.execute_reply.started":"2022-05-29T11:33:04.439842Z","shell.execute_reply":"2022-05-29T11:33:04.439866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs.start_logits.shape,outputs.end_logits.shape,type(outputs.start_logits),type(outputs.start_logits),outputs.start_logits.size(1), # we got one start logit and one end logit for each token in each batch","metadata":{"id":"EDDKFKc9iHJd","outputId":"d6e73bef-3864-47fb-c430-5c315ffd6e24","execution":{"iopub.status.busy":"2022-05-29T11:33:04.441421Z","iopub.status.idle":"2022-05-29T11:33:04.442248Z","shell.execute_reply.started":"2022-05-29T11:33:04.441981Z","shell.execute_reply":"2022-05-29T11:33:04.442009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"oW2Th6B2pGVi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_logits = outputs.start_logits.cpu().numpy()\nend_logits = outputs.end_logits.cpu().numpy()","metadata":{"id":"1o-T0XB9jy3K","execution":{"iopub.status.busy":"2022-05-29T11:33:04.443987Z","iopub.status.idle":"2022-05-29T11:33:04.444532Z","shell.execute_reply.started":"2022-05-29T11:33:04.444245Z","shell.execute_reply":"2022-05-29T11:33:04.44427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_logits[5]","metadata":{"id":"Ud-zRc1YzWxn","outputId":"cbf2f926-44e0-4e94-a700-be25219bcc70","execution":{"iopub.status.busy":"2022-05-29T11:33:04.446016Z","iopub.status.idle":"2022-05-29T11:33:04.446549Z","shell.execute_reply.started":"2022-05-29T11:33:04.446266Z","shell.execute_reply":"2022-05-29T11:33:04.446323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.argsort(start_logits[5])[-1:-20 -1:-1] #[start:end:step]","metadata":{"id":"_tfp8xtDzvYH","outputId":"6f000f18-e60e-4a8b-982e-05114302c221","execution":{"iopub.status.busy":"2022-05-29T11:33:04.447903Z","iopub.status.idle":"2022-05-29T11:33:04.448981Z","shell.execute_reply.started":"2022-05-29T11:33:04.448687Z","shell.execute_reply":"2022-05-29T11:33:04.448713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = np.array([ 2, 0,  1, 5, 4, 10, 9])\na[-1:-4-1]","metadata":{"id":"7fbBtugh1-r_","outputId":"f051aa5c-2a47-4498-9bcb-3d9981f4b447","execution":{"iopub.status.busy":"2022-05-29T11:33:04.450401Z","iopub.status.idle":"2022-05-29T11:33:04.450899Z","shell.execute_reply.started":"2022-05-29T11:33:04.450646Z","shell.execute_reply":"2022-05-29T11:33:04.450671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_set","metadata":{"id":"jeWI82JDkVwU","outputId":"f7104ce5-5884-4c1c-dd24-d926c1c33d20","execution":{"iopub.status.busy":"2022-05-29T11:33:04.452654Z","iopub.status.idle":"2022-05-29T11:33:04.453162Z","shell.execute_reply.started":"2022-05-29T11:33:04.452907Z","shell.execute_reply":"2022-05-29T11:33:04.452932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import collections\n\nexample_to_features = collections.defaultdict(list)\nfor idx, feature in enumerate(eval_set):\n    example_to_features[feature[\"example_id\"]].append(idx)","metadata":{"id":"_C8yAjw-kgmp","execution":{"iopub.status.busy":"2022-05-29T11:33:04.454576Z","iopub.status.idle":"2022-05-29T11:33:04.455477Z","shell.execute_reply.started":"2022-05-29T11:33:04.455185Z","shell.execute_reply":"2022-05-29T11:33:04.455213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_to_features","metadata":{"id":"WLCVs2IVkiVB","outputId":"57d63b00-fceb-41c7-94de-62adac214d73","execution":{"iopub.status.busy":"2022-05-29T11:33:04.457275Z","iopub.status.idle":"2022-05-29T11:33:04.457834Z","shell.execute_reply.started":"2022-05-29T11:33:04.457574Z","shell.execute_reply":"2022-05-29T11:33:04.457601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nn_best = 20\nmax_answer_length = 30\npredicted_answers = []\nfor example in small_eval_set:\n  example_id = example[\"id\"]\n  context = example[\"context\"]\n  answers = []\n  for feature_index in example_to_features[example_id]: \n    start_logit = start_logits[feature_index]\n    end_logit = end_logits[feature_index]\n    offsets = eval_set[\"offset_mapping\"][feature_index]\n    ","metadata":{"id":"hk-KaA1kny3j","outputId":"61b0ef34-706c-4e7c-eb55-df0e1ebb8e03","execution":{"iopub.status.busy":"2022-05-29T11:33:04.459631Z","iopub.status.idle":"2022-05-29T11:33:04.4602Z","shell.execute_reply.started":"2022-05-29T11:33:04.459917Z","shell.execute_reply":"2022-05-29T11:33:04.459954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"1h6u7bfMetPY"}}]}