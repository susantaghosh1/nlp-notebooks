{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install datasets transformers[sentencepiece]\n!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n!pip install scipy sklearn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-01T12:09:38.704291Z","iopub.execute_input":"2022-06-01T12:09:38.705109Z","iopub.status.idle":"2022-06-01T12:10:07.654697Z","shell.execute_reply.started":"2022-06-01T12:09:38.705025Z","shell.execute_reply":"2022-06-01T12:10:07.653607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\nraw_datasets = load_dataset(\"squad\")","metadata":{"execution":{"iopub.status.busy":"2022-06-01T14:05:45.209866Z","iopub.execute_input":"2022-06-01T14:05:45.210439Z","iopub.status.idle":"2022-06-01T14:06:06.849804Z","shell.execute_reply.started":"2022-06-01T14:05:45.210357Z","shell.execute_reply":"2022-06-01T14:06:06.849026Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.97k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbf075c113904bc48995a908c4729c7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d0579a034a54339aef7e3a6163fe069"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"956e113d85d94b91a7da427ace3c36d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/8.12M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19104520741b41aebb56b4cc910b1fb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.05M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67e1860b6688486186e8b23e9a42b199"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ead97a1d6d84aba94e3cf09d19d21d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1593a9f37fce449ab3848d47815b0fa9"}},"metadata":{}}]},{"cell_type":"code","source":"raw_datasets","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:22:04.132508Z","iopub.execute_input":"2022-06-01T12:22:04.132902Z","iopub.status.idle":"2022-06-01T12:22:04.140411Z","shell.execute_reply.started":"2022-06-01T12:22:04.132871Z","shell.execute_reply":"2022-06-01T12:22:04.139271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_checkpoint = \"roberta-large\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T14:06:11.478674Z","iopub.execute_input":"2022-06-01T14:06:11.479430Z","iopub.status.idle":"2022-06-01T14:06:20.066422Z","shell.execute_reply.started":"2022-06-01T14:06:11.479396Z","shell.execute_reply":"2022-06-01T14:06:20.065530Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f55613ad6c440308caf8b3e5ccc5879"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f1e5066c15746eea23971ace6271f43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30b055a9080241eb9c6c3fdd3727a8a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91bdae49174a44089dbdff8e20203a28"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.is_fast","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:11:07.940933Z","iopub.execute_input":"2022-06-01T12:11:07.941334Z","iopub.status.idle":"2022-06-01T12:11:07.947085Z","shell.execute_reply.started":"2022-06-01T12:11:07.941298Z","shell.execute_reply":"2022-06-01T12:11:07.946161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = 384\nstride = 128\n\n\ndef preprocess_training_examples(examples):\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    offset_mapping = inputs.pop(\"offset_mapping\")\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    answers = examples[\"answers\"]\n    start_positions = []\n    end_positions = []\n\n    for i, offset in enumerate(offset_mapping):\n        sample_idx = sample_map[i]\n        answer = answers[sample_idx]\n        start_char = answer[\"answer_start\"][0]\n        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n        sequence_ids = inputs.sequence_ids(i)\n\n        # Find the start and end of the context\n        idx = 0\n        while sequence_ids[idx] != 1:\n            idx += 1\n        context_start = idx\n        while sequence_ids[idx] == 1:\n            idx += 1\n        context_end = idx - 1\n\n        # If the answer is not fully inside the context, label is (0, 0)\n        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            # Otherwise it's the start and end token positions\n            idx = context_start\n            while idx <= context_end and offset[idx][0] <= start_char:\n                idx += 1\n            start_positions.append(idx - 1)\n\n            idx = context_end\n            while idx >= context_start and offset[idx][1] >= end_char:\n                idx -= 1\n            end_positions.append(idx + 1)\n\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2022-06-01T14:06:22.956586Z","iopub.execute_input":"2022-06-01T14:06:22.957432Z","iopub.status.idle":"2022-06-01T14:06:22.970115Z","shell.execute_reply.started":"2022-06-01T14:06:22.957396Z","shell.execute_reply":"2022-06-01T14:06:22.969314Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_dataset = raw_datasets[\"train\"].map(\n    preprocess_training_examples,\n    batched=True,\n    remove_columns=raw_datasets[\"train\"].column_names,\n)\nlen(raw_datasets[\"train\"]), len(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T14:06:26.941167Z","iopub.execute_input":"2022-06-01T14:06:26.941539Z","iopub.status.idle":"2022-06-01T14:07:43.802789Z","shell.execute_reply.started":"2022-06-01T14:06:26.941508Z","shell.execute_reply":"2022-06-01T14:07:43.801836Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/88 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78f47315b4134d09b98731d629aa7400"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(87599, 88567)"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_validation_examples(examples):\n    questions = [q.strip() for q in examples[\"question\"]]\n    inputs = tokenizer(\n        questions,\n        examples[\"context\"],\n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n    example_ids = []\n\n    for i in range(len(inputs[\"input_ids\"])):\n        sample_idx = sample_map[i]\n        example_ids.append(examples[\"id\"][sample_idx])\n\n        sequence_ids = inputs.sequence_ids(i)\n        offset = inputs[\"offset_mapping\"][i]\n        inputs[\"offset_mapping\"][i] = [\n            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n        ]\n\n    inputs[\"example_id\"] = example_ids\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2022-06-01T14:07:55.165244Z","iopub.execute_input":"2022-06-01T14:07:55.165611Z","iopub.status.idle":"2022-06-01T14:07:55.173395Z","shell.execute_reply.started":"2022-06-01T14:07:55.165581Z","shell.execute_reply":"2022-06-01T14:07:55.172649Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"validation_dataset = raw_datasets[\"validation\"].map(\n    preprocess_validation_examples,\n    batched=True,\n    remove_columns=raw_datasets[\"validation\"].column_names,\n)\nlen(raw_datasets[\"validation\"]), len(validation_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T14:07:57.835517Z","iopub.execute_input":"2022-06-01T14:07:57.836166Z","iopub.status.idle":"2022-06-01T14:09:01.005740Z","shell.execute_reply.started":"2022-06-01T14:07:57.836132Z","shell.execute_reply":"2022-06-01T14:09:01.004908Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/11 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67d37d75a5a44a6eb34377e4c94bc250"}},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(10570, 10790)"},"metadata":{}}]},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport collections\n\ndef compute_metrics(start_logits, end_logits, features, examples):\n    example_to_features = collections.defaultdict(list)\n    for idx, feature in enumerate(features):\n        example_to_features[feature[\"example_id\"]].append(idx)\n\n    predicted_answers = []\n    for example in tqdm(examples):\n        example_id = example[\"id\"]\n        context = example[\"context\"]\n        answers = []\n\n        # Loop through all features associated with that example\n        for feature_index in example_to_features[example_id]:\n            start_logit = start_logits[feature_index]\n            end_logit = end_logits[feature_index]\n            offsets = features[feature_index][\"offset_mapping\"]\n\n            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Skip answers that are not fully in the context\n                    if offsets[start_index] is None or offsets[end_index] is None:\n                        continue\n                    # Skip answers with a length that is either < 0 or > max_answer_length\n                    if (\n                        end_index < start_index\n                        or end_index - start_index + 1 > max_answer_length\n                    ):\n                        continue\n\n                    answer = {\n                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n                    }\n                    answers.append(answer)\n\n        # Select the answer with the best score\n        if len(answers) > 0:\n            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n            predicted_answers.append(\n                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n            )\n        else:\n            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n\n    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n    return metric.compute(predictions=predicted_answers, references=theoretical_answers)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T01:31:23.122742Z","iopub.execute_input":"2022-06-02T01:31:23.123395Z","iopub.status.idle":"2022-06-02T01:31:23.249161Z","shell.execute_reply.started":"2022-06-02T01:31:23.123288Z","shell.execute_reply":"2022-06-02T01:31:23.248372Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForQuestionAnswering\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T14:09:27.889681Z","iopub.execute_input":"2022-06-01T14:09:27.890070Z","iopub.status.idle":"2022-06-01T14:10:05.015870Z","shell.execute_reply.started":"2022-06-01T14:09:27.890041Z","shell.execute_reply":"2022-06-01T14:10:05.014838Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38810ed6ced74693b22ed1d92d3c0063"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForQuestionAnswering: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T14:10:23.620418Z","iopub.execute_input":"2022-06-01T14:10:23.621277Z","iopub.status.idle":"2022-06-01T14:10:23.728186Z","shell.execute_reply.started":"2022-06-01T14:10:23.621238Z","shell.execute_reply":"2022-06-01T14:10:23.727338Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a55bae6386e4bb1ad579f04b5461305"}},"metadata":{}}]},{"cell_type":"code","source":"\n!apt install git-lfs","metadata":{"execution":{"iopub.status.busy":"2022-06-01T14:10:41.246014Z","iopub.execute_input":"2022-06-01T14:10:41.247046Z","iopub.status.idle":"2022-06-01T14:10:47.320553Z","shell.execute_reply.started":"2022-06-01T14:10:41.247001Z","shell.execute_reply":"2022-06-01T14:10:47.319427Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following NEW packages will be installed:\n  git-lfs\n0 upgraded, 1 newly installed, 0 to remove and 27 not upgraded.\nNeed to get 3316 kB of archives.\nAfter this operation, 11.1 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 git-lfs amd64 2.9.2-1 [3316 kB]\nFetched 3316 kB in 1s (2741 kB/s) \u001b[0m[33m\u001b[33m\n\n\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package git-lfs.\n(Reading database ... 108086 files and directories currently installed.)\nPreparing to unpack .../git-lfs_2.9.2-1_amd64.deb ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking git-lfs (2.9.2-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Setting up git-lfs (2.9.2-1) ...\n\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8Processing triggers for man-db (2.9.1-1) ...\n\n\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\nargs = TrainingArguments(\n    \"roberta-large-squad\",\n    evaluation_strategy=\"no\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    fp16=True,\n    push_to_hub=True,\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-01T14:11:11.585078Z","iopub.execute_input":"2022-06-01T14:11:11.585914Z","iopub.status.idle":"2022-06-01T14:11:11.667898Z","shell.execute_reply.started":"2022-06-01T14:11:11.585872Z","shell.execute_reply":"2022-06-01T14:11:11.667167Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}]},{"cell_type":"code","source":"%%capture\n!ip install wandb","metadata":{"execution":{"iopub.status.busy":"2022-06-01T12:20:24.087248Z","iopub.execute_input":"2022-06-01T12:20:24.088182Z","iopub.status.idle":"2022-06-01T12:20:24.84638Z","shell.execute_reply.started":"2022-06-01T12:20:24.088139Z","shell.execute_reply":"2022-06-01T12:20:24.845391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2022-06-01T14:10:50.010879Z","iopub.execute_input":"2022-06-01T14:10:50.011581Z","iopub.status.idle":"2022-06-01T14:10:50.018904Z","shell.execute_reply.started":"2022-06-01T14:10:50.011540Z","shell.execute_reply":"2022-06-01T14:10:50.017873Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    tokenizer=tokenizer,\n)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-06-01T14:11:21.580432Z","iopub.execute_input":"2022-06-01T14:11:21.580841Z","iopub.status.idle":"2022-06-01T20:54:12.585365Z","shell.execute_reply.started":"2022-06-01T14:11:21.580810Z","shell.execute_reply":"2022-06-01T20:54:12.584296Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"Cloning https://huggingface.co/susghosh/roberta-large-squad into local empty directory.\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"Using amp half precision backend\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n***** Running training *****\n  Num examples = 88567\n  Num Epochs = 2\n  Instantaneous batch size per device = 8\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 1\n  Total optimization steps = 22142\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='22142' max='22142' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [22142/22142 6:42:35, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.651600</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.086100</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.066500</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.036300</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.988100</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.949000</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.897000</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.920100</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.898300</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.860200</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.863200</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.870400</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.863500</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.828900</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.847800</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.823000</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.834000</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.834200</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.801300</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.788200</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.804200</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.790000</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.598300</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.611400</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.615800</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.598200</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.571800</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.587900</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.589700</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.554600</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>0.564400</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.581100</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>0.578500</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>0.544500</td>\n    </tr>\n    <tr>\n      <td>17500</td>\n      <td>0.553500</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>0.567400</td>\n    </tr>\n    <tr>\n      <td>18500</td>\n      <td>0.602700</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>0.546200</td>\n    </tr>\n    <tr>\n      <td>19500</td>\n      <td>0.525800</td>\n    </tr>\n    <tr>\n      <td>20000</td>\n      <td>0.556500</td>\n    </tr>\n    <tr>\n      <td>20500</td>\n      <td>0.573200</td>\n    </tr>\n    <tr>\n      <td>21000</td>\n      <td>0.534700</td>\n    </tr>\n    <tr>\n      <td>21500</td>\n      <td>0.553100</td>\n    </tr>\n    <tr>\n      <td>22000</td>\n      <td>0.548000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to roberta-large-squad/checkpoint-11071\nConfiguration saved in roberta-large-squad/checkpoint-11071/config.json\nModel weights saved in roberta-large-squad/checkpoint-11071/pytorch_model.bin\ntokenizer config file saved in roberta-large-squad/checkpoint-11071/tokenizer_config.json\nSpecial tokens file saved in roberta-large-squad/checkpoint-11071/special_tokens_map.json\ntokenizer config file saved in roberta-large-squad/tokenizer_config.json\nSpecial tokens file saved in roberta-large-squad/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"Saving model checkpoint to roberta-large-squad/checkpoint-22142\nConfiguration saved in roberta-large-squad/checkpoint-22142/config.json\nModel weights saved in roberta-large-squad/checkpoint-22142/pytorch_model.bin\ntokenizer config file saved in roberta-large-squad/checkpoint-22142/tokenizer_config.json\nSpecial tokens file saved in roberta-large-squad/checkpoint-22142/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"tokenizer config file saved in roberta-large-squad/tokenizer_config.json\nSpecial tokens file saved in roberta-large-squad/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=22142, training_loss=0.7454382929415596, metrics={'train_runtime': 24157.727, 'train_samples_per_second': 7.332, 'train_steps_per_second': 0.917, 'total_flos': 1.2337918825770086e+17, 'train_loss': 0.7454382929415596, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"code","source":"predictions, _, _ = trainer.predict(validation_dataset)\nstart_logits, end_logits = predictions\ncompute_metrics(start_logits, end_logits, validation_dataset, raw_datasets[\"validation\"])","metadata":{"execution":{"iopub.status.busy":"2022-06-02T01:32:01.378038Z","iopub.execute_input":"2022-06-02T01:32:01.378415Z","iopub.status.idle":"2022-06-02T01:32:01.444813Z","shell.execute_reply.started":"2022-06-02T01:32:01.378382Z","shell.execute_reply":"2022-06-02T01:32:01.443553Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_50/1334907046.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstart_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"],"ename":"NameError","evalue":"name 'trainer' is not defined","output_type":"error"}]},{"cell_type":"code","source":"trainer.push_to_hub(commit_message=\"Training complete\")","metadata":{"execution":{"iopub.status.busy":"2022-06-01T21:02:09.922397Z","iopub.status.idle":"2022-06-01T21:02:09.923070Z","shell.execute_reply.started":"2022-06-01T21:02:09.922794Z","shell.execute_reply":"2022-06-01T21:02:09.922821Z"},"trusted":true},"execution_count":null,"outputs":[]}]}