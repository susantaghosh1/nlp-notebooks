{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afdccdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets transformers[sentencepiece]\n",
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "!pip install scipy sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b09e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60cbdb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c524afad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db80c481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 17 09:04:43 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 516.01       Driver Version: 516.01       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   51C    P8     5W /  N/A |    419MiB /  6144MiB |      3%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      4228    C+G   ...Browser\\CtxWebBrowser.exe    N/A      |\n",
      "|    0   N/A  N/A      4392    C+G   ....0.18\\OverwolfBrowser.exe    N/A      |\n",
      "|    0   N/A  N/A      4524    C+G   ...icePlugin\\SelfService.exe    N/A      |\n",
      "|    0   N/A  N/A      7920    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      9748    C+G   ...n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9772    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9952    C+G   ...210.47\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     10580    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     10792    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     11076    C+G   ...2gh52qy24etm\\Nahimic3.exe    N/A      |\n",
      "|    0   N/A  N/A     11124    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     14952    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
      "|    0   N/A  N/A     15244    C+G   ...86)\\Overwolf\\Overwolf.exe    N/A      |\n",
      "|    0   N/A  N/A     15332    C+G   ...h8wxbdkxb8p\\DCv2\\DCv2.exe    N/A      |\n",
      "|    0   N/A  N/A     16536    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A     16796    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     17568    C+G   ...210.47\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     18348    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d84c49dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.19.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db20b1",
   "metadata": {},
   "source": [
    "# Fine-tuning a model on a text classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c26da4",
   "metadata": {},
   "source": [
    "In this notebook, we will see how to fine-tune one of the ðŸ¤— Transformers model to a text classification task of the GLUE Benchmark.\n",
    "\n",
    "The GLUE Benchmark is a group of nine classification tasks on sentences or pairs of sentences which are:\n",
    "\n",
    "CoLA (Corpus of Linguistic Acceptability) : Determine if a sentence is grammatically correct or not.is a dataset containing sentences labeled grammatically correct or not.\n",
    "\n",
    "MNLI (Multi-Genre Natural Language Inference) : Determine if a sentence entails, contradicts or is unrelated to a given hypothesis. (This dataset has two versions, one with the validation and test set coming from the same distribution, another called mismatched where the validation and test use out-of-domain data.)\n",
    "\n",
    "MRPC (Microsoft Research Paraphrase Corpus) : Determine if two sentences are paraphrases from one another or not.\n",
    "\n",
    "QNLI (Question-answering Natural Language Inference) : Determine if the answer to a question is in the second sentence or not. (This dataset is built from the SQuAD dataset.)\n",
    "\n",
    "QQP (Quora Question Pairs2) : Determine if two questions are semantically equivalent or not.\n",
    "\n",
    "RTE (Recognizing Textual Entailment) Determine if a sentence entails a given hypothesis or not.\n",
    "\n",
    "SST-2 (Stanford Sentiment Treebank) Determine if the sentence has a positive or negative sentiment.\n",
    "\n",
    "STS-B (Semantic Textual Similarity Benchmark) Determine the similarity of two sentences with a score from 1 to 5.\n",
    "\n",
    "WNLI (Winograd Natural Language Inference) Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not. (This dataset is built from the Winograd Schema Challenge dataset.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e293a3",
   "metadata": {},
   "source": [
    "We will see how to easily load the dataset for each one of those tasks and use the Trainer API to fine-tune a model on it. Each task is named by its acronym, with mnli-mm standing for the mismatched version of MNLI (so same training set as mnli but different validation and test sets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a413150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c6ddeb",
   "metadata": {},
   "source": [
    "This notebook is built to run on any of the tasks in the list above, with any model checkpoint from the Model Hub as long as that model has a version with a classification head. Depending on you model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those three parameters, then the rest of the notebook should run smoothly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de7d6704",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"mnli\"\n",
    "model_checkpoint = \"distilbert-base-uncased\" ##choose distilbert to reduce training time. bert,roberta,deberta all the models based on transformer encoder architecture with classification head on top can be used\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef6461",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c954bacf",
   "metadata": {},
   "source": [
    "We will use the ðŸ¤— Datasets library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions load_dataset and load_metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20e82fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf62b8f",
   "metadata": {},
   "source": [
    "Apart from mnli-mm being a special code, we can directly pass our task name to those functions. load_dataset will cache the dataset to avoid downloading it again the next time you run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41cd60f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to load mnli dataset as well as metric for evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\ghosh\\.cache\\huggingface\\datasets\\glue\\mnli\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe20e157f41429a9ee586a0811535b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "print(f\"going to load {actual_task} dataset as well as metric for evaluation\")\n",
    "dataset = load_dataset('glue',actual_task)\n",
    "metric = load_metric('glue',actual_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e1e888f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset : DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 392702\n",
      "    })\n",
      "    validation_matched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9815\n",
      "    })\n",
      "    validation_mismatched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9832\n",
      "    })\n",
      "    test_matched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9796\n",
      "    })\n",
      "    test_mismatched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9847\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"loaded dataset : {dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75031468",
   "metadata": {},
   "source": [
    "let's explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5a2fd93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': Value(dtype='string', id=None),\n",
       " 'hypothesis': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=3, names=['entailment', 'neutral', 'contradiction'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features #features of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b7999b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "integer label for  entailment :: 0\n",
      "integer label for  neutral :: 1\n",
      "integer label for  contradiction :: 2\n"
     ]
    }
   ],
   "source": [
    "#let's print integer label corresponda to string labels\n",
    "for _ in ['entailment', 'neutral', 'contradiction']:\n",
    "    print(f\"integer label for  {_} :: {dataset['train'].features['label'].str2int(_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "31c79e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.',\n",
       " 'hypothesis': 'Product and geography are what make cream skimming work. ',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0] # first element of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "68a7a7b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': ['Conceptually cream skimming has two basic dimensions - product and geography.',\n",
       "  'you know during the season and i guess at at your level uh you lose them to the next level if if they decide to recall the the parent team the Braves decide to call to recall a guy from triple A then a double A guy goes up to replace him and a single A guy goes up to replace him',\n",
       "  'One of our number will carry out your instructions minutely.',\n",
       "  'How do you know? All this is their information again.',\n",
       "  \"yeah i tell you what though if you go price some of those tennis shoes i can see why now you know they're getting up in the hundred dollar range\"],\n",
       " 'hypothesis': ['Product and geography are what make cream skimming work. ',\n",
       "  'You lose the things to the following level if the people recall.',\n",
       "  'A member of my team will execute your orders with immense precision.',\n",
       "  'This information belongs to them.',\n",
       "  'The tennis shoes have a range of prices.'],\n",
       " 'label': [1, 0, 0, 0, 1],\n",
       " 'idx': [0, 1, 2, 3, 4]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "96fe95cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['premise', 'hypothesis', 'label', 'idx'])\n",
      "shape of premise : 5\n",
      "shape of hypothesis : 5\n",
      "shape of label : 5\n",
      "shape of idx : 5\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][:5].keys())\n",
    "for k,v in dataset['train'][:5].items():\n",
    "    print(f\"shape of {k} : {len(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "97894cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"glue\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
       "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
       "Args:\n",
       "    predictions: list of predictions to score.\n",
       "        Each translation should be tokenized into a list of tokens.\n",
       "    references: list of lists of references for each translation.\n",
       "        Each reference should be tokenized into a list of tokens.\n",
       "Returns: depending on the GLUE subset, one or several of:\n",
       "    \"accuracy\": Accuracy\n",
       "    \"f1\": F1 score\n",
       "    \"pearson\": Pearson Correlation\n",
       "    \"spearmanr\": Spearman Correlation\n",
       "    \"matthews_correlation\": Matthew Correlation\n",
       "Examples:\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0, 'f1': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'stsb')\n",
       "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
       "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'cola')\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'matthews_correlation': 1.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585d452",
   "metadata": {},
   "source": [
    "You can call its compute method with your predictions and labels directly and it will return a dictionary with the metric(s) value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3876d69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.compute?? ## this is how you need to see the documentation in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5c3e8f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.40625}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "fake_preds = np.random.randint(0, 3, size=(64,)) # as in MNLI dataset we have 3 labels, so here I am generating 64 numbers between 0 and 3\n",
    "fake_labels = np.random.randint(0, 3, size=(64,))\n",
    "metric.compute(predictions=fake_preds,references=fake_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d02d324",
   "metadata": {},
   "source": [
    "Note that load_metric has loaded the proper metric associated to your task, which is:\n",
    "\n",
    "for CoLA: Matthews Correlation Coefficient\n",
    "\n",
    "for MNLI (matched or mismatched): Accuracy\n",
    "for MRPC: Accuracy and F1 score\n",
    "for QNLI: Accuracy\n",
    "for QQP: Accuracy and F1 score\n",
    "for RTE: Accuracy\n",
    "for SST-2: Accuracy\n",
    "for STS-B: Pearson Correlation Coefficient and Spearman's_Rank_Correlation_Coefficient\n",
    "for WNLI: Accuracy\n",
    "so the metric object only computes the one(s) needed for your task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3551a18a",
   "metadata": {},
   "source": [
    "# Tokenizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc133299",
   "metadata": {},
   "source": [
    "Before we can feed those texts to our model, we need to preprocess them. This is done by a ðŸ¤— Transformers Tokenizer which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
    "\n",
    "To do all of this, we instantiate our tokenizer with the AutoTokenizer.from_pretrained method, which will ensure:\n",
    "\n",
    "we get a tokenizer that corresponds to the model architecture we want to use,\n",
    "we download the vocabulary used when pretraining this specific checkpoint.\n",
    "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc674415",
   "metadata": {},
   "source": [
    "Pls note : if you have domain specific corpus or your fine-tuning data varies too much with respect to pre-training data of the model [like BERT was pretrained with wikipedia and book corpus with masked language modelling [MLM] and next sentence prediction [NSP] task] then it's recommended to first train your tokenizer [Bert-like [wordpiece tokenizer which bert uses]] on your domain data and then do a re-training of BERT with the trained tokenizer on Masked language Modelling [MLM] task. This is called domain adoptaion of a pre-trained language model. Then you finetune that domain adopted model in any of the downstream tasks like BERT can be fine-tuned to sequence or document classification [binary/multi-class/multi-label], token classification or Named Entity Recognition [NER] , Extractive Question Answering [ Single Span], multiple Choice Question  Answering ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1bc82957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7c91181fc83465f8139b6930065b550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec21ad52cb00451b920f2a65b65ed5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01982a6f76bf42bda517fb7850544b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c5f674995b410cb2ebd371efb2294b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint,use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa36c37",
   "metadata": {},
   "source": [
    "We pass along use_fast=True to the call above to use one of the fast tokenizers (backed by Rust) from the ðŸ¤— Tokenizers library. Those fast tokenizers are available for almost all models, but if you got an error with the previous call, remove that argument.\n",
    "\n",
    "You can directly call this tokenizer on one sentence or a pair of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e6269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
