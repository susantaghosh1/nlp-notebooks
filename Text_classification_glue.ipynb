{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c4fa35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets transformers[sentencepiece]\n",
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "!pip install scipy sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b09e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60cbdb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c524afad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db80c481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 17 09:04:43 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 516.01       Driver Version: 516.01       CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   51C    P8     5W /  N/A |    419MiB /  6144MiB |      3%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      4228    C+G   ...Browser\\CtxWebBrowser.exe    N/A      |\n",
      "|    0   N/A  N/A      4392    C+G   ....0.18\\OverwolfBrowser.exe    N/A      |\n",
      "|    0   N/A  N/A      4524    C+G   ...icePlugin\\SelfService.exe    N/A      |\n",
      "|    0   N/A  N/A      7920    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      9748    C+G   ...n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9772    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9952    C+G   ...210.47\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     10580    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     10792    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     11076    C+G   ...2gh52qy24etm\\Nahimic3.exe    N/A      |\n",
      "|    0   N/A  N/A     11124    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     14952    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
      "|    0   N/A  N/A     15244    C+G   ...86)\\Overwolf\\Overwolf.exe    N/A      |\n",
      "|    0   N/A  N/A     15332    C+G   ...h8wxbdkxb8p\\DCv2\\DCv2.exe    N/A      |\n",
      "|    0   N/A  N/A     16536    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A     16796    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     17568    C+G   ...210.47\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     18348    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d84c49dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.19.1\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e8a069",
   "metadata": {},
   "source": [
    "# Fine-tuning a model on a text classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d1ead0",
   "metadata": {},
   "source": [
    "In this notebook, we will see how to fine-tune one of the ðŸ¤— Transformers model to a text classification task of the GLUE Benchmark.\n",
    "\n",
    "The GLUE Benchmark is a group of nine classification tasks on sentences or pairs of sentences which are:\n",
    "\n",
    "CoLA (Corpus of Linguistic Acceptability) : Determine if a sentence is grammatically correct or not.is a dataset containing sentences labeled grammatically correct or not.\n",
    "\n",
    "MNLI (Multi-Genre Natural Language Inference) : Determine if a sentence entails, contradicts or is unrelated to a given hypothesis. (This dataset has two versions, one with the validation and test set coming from the same distribution, another called mismatched where the validation and test use out-of-domain data.)\n",
    "\n",
    "MRPC (Microsoft Research Paraphrase Corpus) : Determine if two sentences are paraphrases from one another or not.\n",
    "\n",
    "QNLI (Question-answering Natural Language Inference) : Determine if the answer to a question is in the second sentence or not. (This dataset is built from the SQuAD dataset.)\n",
    "\n",
    "QQP (Quora Question Pairs2) : Determine if two questions are semantically equivalent or not.\n",
    "\n",
    "RTE (Recognizing Textual Entailment) Determine if a sentence entails a given hypothesis or not.\n",
    "\n",
    "SST-2 (Stanford Sentiment Treebank) Determine if the sentence has a positive or negative sentiment.\n",
    "\n",
    "STS-B (Semantic Textual Similarity Benchmark) Determine the similarity of two sentences with a score from 1 to 5.\n",
    "\n",
    "WNLI (Winograd Natural Language Inference) Determine if a sentence with an anonymous pronoun and a sentence with this pronoun replaced are entailed or not. (This dataset is built from the Winograd Schema Challenge dataset.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a75669",
   "metadata": {},
   "source": [
    "We will see how to easily load the dataset for each one of those tasks and use the Trainer API to fine-tune a model on it. Each task is named by its acronym, with mnli-mm standing for the mismatched version of MNLI (so same training set as mnli but different validation and test sets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c68c16cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7a6a0f",
   "metadata": {},
   "source": [
    "This notebook is built to run on any of the tasks in the list above, with any model checkpoint from the Model Hub as long as that model has a version with a classification head. Depending on you model and the GPU you are using, you might need to adjust the batch size to avoid out-of-memory errors. Set those three parameters, then the rest of the notebook should run smoothly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98438487",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"mnli\"\n",
    "model_checkpoint = \"distilbert-base-uncased\" ##choose distilbert to reduce training time. bert,roberta,deberta all the models based on transformer encoder architecture with classification head on top can be used\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c26d69",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd2de42",
   "metadata": {},
   "source": [
    "We will use the ðŸ¤— Datasets library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions load_dataset and load_metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af487903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1598a642",
   "metadata": {},
   "source": [
    "Apart from mnli-mm being a special code, we can directly pass our task name to those functions. load_dataset will cache the dataset to avoid downloading it again the next time you run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b25e2955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to load mnli dataset as well as metric for evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\ghosh\\.cache\\huggingface\\datasets\\glue\\mnli\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe20e157f41429a9ee586a0811535b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "print(f\"going to load {actual_task} dataset as well as metric for evaluation\")\n",
    "dataset = load_dataset('glue',actual_task)\n",
    "metric = load_metric('glue',actual_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1165eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded dataset : DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 392702\n",
      "    })\n",
      "    validation_matched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9815\n",
      "    })\n",
      "    validation_mismatched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9832\n",
      "    })\n",
      "    test_matched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9796\n",
      "    })\n",
      "    test_mismatched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9847\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"loaded dataset : {dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26674227",
   "metadata": {},
   "source": [
    "let's explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5caa9fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': Value(dtype='string', id=None),\n",
       " 'hypothesis': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=3, names=['entailment', 'neutral', 'contradiction'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'].features #features of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6d1de2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "integer label for  entailment :: 0\n",
      "integer label for  neutral :: 1\n",
      "integer label for  contradiction :: 2\n"
     ]
    }
   ],
   "source": [
    "#let's print integer label corresponda to string labels\n",
    "for _ in ['entailment', 'neutral', 'contradiction']:\n",
    "    print(f\"integer label for  {_} :: {dataset['train'].features['label'].str2int(_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e9be5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.',\n",
       " 'hypothesis': 'Product and geography are what make cream skimming work. ',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0] # first element of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f0e679f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': ['Conceptually cream skimming has two basic dimensions - product and geography.',\n",
       "  'you know during the season and i guess at at your level uh you lose them to the next level if if they decide to recall the the parent team the Braves decide to call to recall a guy from triple A then a double A guy goes up to replace him and a single A guy goes up to replace him',\n",
       "  'One of our number will carry out your instructions minutely.',\n",
       "  'How do you know? All this is their information again.',\n",
       "  \"yeah i tell you what though if you go price some of those tennis shoes i can see why now you know they're getting up in the hundred dollar range\"],\n",
       " 'hypothesis': ['Product and geography are what make cream skimming work. ',\n",
       "  'You lose the things to the following level if the people recall.',\n",
       "  'A member of my team will execute your orders with immense precision.',\n",
       "  'This information belongs to them.',\n",
       "  'The tennis shoes have a range of prices.'],\n",
       " 'label': [1, 0, 0, 0, 1],\n",
       " 'idx': [0, 1, 2, 3, 4]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4be353fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['premise', 'hypothesis', 'label', 'idx'])\n",
      "shape of premise : 5\n",
      "shape of hypothesis : 5\n",
      "shape of label : 5\n",
      "shape of idx : 5\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][:5].keys())\n",
    "for k,v in dataset['train'][:5].items():\n",
    "    print(f\"shape of {k} : {len(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2b8c6c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"glue\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
       "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
       "Args:\n",
       "    predictions: list of predictions to score.\n",
       "        Each translation should be tokenized into a list of tokens.\n",
       "    references: list of lists of references for each translation.\n",
       "        Each reference should be tokenized into a list of tokens.\n",
       "Returns: depending on the GLUE subset, one or several of:\n",
       "    \"accuracy\": Accuracy\n",
       "    \"f1\": F1 score\n",
       "    \"pearson\": Pearson Correlation\n",
       "    \"spearmanr\": Spearman Correlation\n",
       "    \"matthews_correlation\": Matthew Correlation\n",
       "Examples:\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0, 'f1': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'stsb')\n",
       "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
       "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'cola')\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'matthews_correlation': 1.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7669b8a2",
   "metadata": {},
   "source": [
    "You can call its compute method with your predictions and labels directly and it will return a dictionary with the metric(s) value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b2a94e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.compute?? ## this is how you need to see the documentation in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0d0d1815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fake_preds = np.random.randint(0, 3, size=(64,)) # as in MNLI dataset we have 3 labels, so here I am generating 64 numbers between 0 and 3\n",
    "fake_labels = np.random.randint(0, 3, size=(64,))\n",
    "metric.compute(predictions=fake_preds,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b306e33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
