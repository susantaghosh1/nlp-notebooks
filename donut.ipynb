{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNV9ItOHzEUhjVCb72xwS54",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/susantaghosh1/nlp-notebooks/blob/main/donut.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnaGV1J_AObq"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "def add_tokens(list_of_tokens: List[str]):\n",
        "    \"\"\"\n",
        "    Add tokens to tokenizer and resize the token embeddings\n",
        "    \"\"\"\n",
        "    newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)\n",
        "    if newly_added_num > 0:\n",
        "        model.decoder.resize_token_embeddings(len(processor.tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "additional_tokens = [\"<advertisement/>\", \"<budget/>\", \"<email/>\", \"<file_folder/>\", \"<form/>\", \"<handwritten/>\", \"<invoice/>\",\n",
        "  \"<letter/>\", \"<memo/>\", \"<news_article/>\", \"<presentation/>\", \"<questionnaire/>\", \"<resume/>\",\n",
        "  \"<scientific_publication/>\", \"<scientific_report/>\", \"<specification/>\"]\n",
        "\n",
        "add_tokens(additional_tokens)"
      ],
      "metadata": {
        "id": "dKlYgys3AYFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "# import random\n",
        "# from typing import Any, List, Tuple\n",
        "\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset\n",
        "\n",
        "# class DonutDataset(Dataset):\n",
        "#     \"\"\"\n",
        "#     DonutDataset which is saved in huggingface datasets format. (see details in https://huggingface.co/docs/datasets)\n",
        "#     Each row, consists of image path(png/jpg/jpeg) and gt data (json/jsonl/txt),\n",
        "#     and it will be converted into input_tensor(vectorized image) and input_ids(tokenized string).\n",
        "#     Args:\n",
        "#         dataset_name_or_path: name of dataset (available at huggingface.co/datasets) or the path containing image files and metadata.jsonl\n",
        "#         max_length: the max number of tokens for the target sequences\n",
        "#         split: whether to load \"train\", \"validation\" or \"test\" split\n",
        "#         ignore_id: ignore_index for torch.nn.CrossEntropyLoss\n",
        "#         task_start_token: the special token to be fed to the decoder to conduct the target task\n",
        "#         prompt_end_token: the special token at the end of the sequences\n",
        "#         sort_json_key: whether or not to sort the JSON keys\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         dataset_name_or_path: str,\n",
        "#         max_length: int,\n",
        "#         split: str = \"train\",\n",
        "#         ignore_id: int = -100,\n",
        "#         task_start_token: str = \"<s>\",\n",
        "#         prompt_end_token: str = None,\n",
        "#         sort_json_key: bool = True,\n",
        "#     ):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.max_length = max_length\n",
        "#         self.split = split\n",
        "#         self.ignore_id = ignore_id\n",
        "#         self.task_start_token = task_start_token\n",
        "#         self.prompt_end_token = prompt_end_token if prompt_end_token else task_start_token\n",
        "#         self.sort_json_key = sort_json_key\n",
        "\n",
        "#         self.dataset = load_dataset(dataset_name_or_path, split=self.split)\n",
        "#         self.dataset_length = len(self.dataset)\n",
        "\n",
        "#         self.gt_token_sequences = []\n",
        "#         for sample in self.dataset:\n",
        "#             ground_truth = json.loads(sample[\"ground_truth\"])\n",
        "#             if \"gt_parses\" in ground_truth:  # when multiple ground truths are available, e.g., docvqa\n",
        "#                 assert isinstance(ground_truth[\"gt_parses\"], list)\n",
        "#                 gt_jsons = ground_truth[\"gt_parses\"]\n",
        "#             else:\n",
        "#                 assert \"gt_parse\" in ground_truth and isinstance(ground_truth[\"gt_parse\"], dict)\n",
        "#                 gt_jsons = [ground_truth[\"gt_parse\"]]\n",
        "\n",
        "#             self.gt_token_sequences.append(\n",
        "#                 [\n",
        "#                     self.json2token(\n",
        "#                         gt_json,\n",
        "#                         update_special_tokens_for_json_key=self.split == \"train\",\n",
        "#                         sort_json_key=self.sort_json_key,\n",
        "#                     )\n",
        "#                     + processor.tokenizer.eos_token\n",
        "#                     for gt_json in gt_jsons  # load json from list of json\n",
        "#                 ]\n",
        "#             )\n",
        "\n",
        "#         self.add_tokens([self.task_start_token, self.prompt_end_token])\n",
        "#         self.prompt_end_token_id = processor.tokenizer.convert_tokens_to_ids(self.prompt_end_token)\n",
        "\n",
        "#     def json2token(self, obj: Any, update_special_tokens_for_json_key: bool = True, sort_json_key: bool = True):\n",
        "#         \"\"\"\n",
        "#         Convert an ordered JSON object into a token sequence\n",
        "#         \"\"\"\n",
        "#         if type(obj) == dict:\n",
        "#             if len(obj) == 1 and \"text_sequence\" in obj:\n",
        "#                 return obj[\"text_sequence\"]\n",
        "#             else:\n",
        "#                 output = \"\"\n",
        "#                 if sort_json_key:\n",
        "#                     keys = sorted(obj.keys(), reverse=True)\n",
        "#                 else:\n",
        "#                     keys = obj.keys()\n",
        "#                 for k in keys:\n",
        "#                     if update_special_tokens_for_json_key:\n",
        "#                         self.add_tokens([fr\"<s_{k}>\", fr\"</s_{k}>\"])\n",
        "#                     output += (\n",
        "#                         fr\"<s_{k}>\"\n",
        "#                         + self.json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)\n",
        "#                         + fr\"</s_{k}>\"\n",
        "#                     )\n",
        "#                 return output\n",
        "#         elif type(obj) == list:\n",
        "#             return r\"<sep/>\".join(\n",
        "#                 [self.json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]\n",
        "#             )\n",
        "#         else:\n",
        "#             obj = str(obj)\n",
        "#             if f\"<{obj}/>\" in additional_tokens:\n",
        "#                 obj = f\"<{obj}/>\"  # for categorical special tokens\n",
        "#             return obj\n",
        "    \n",
        "#     def add_tokens(self, list_of_tokens: List[str]):\n",
        "#         \"\"\"\n",
        "#         Add tokens to tokenizer and resize the token embeddings of the decoder\n",
        "#         \"\"\"\n",
        "#         newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)\n",
        "#         if newly_added_num > 0:\n",
        "#             model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
        "    \n",
        "#     def __len__(self) -> int:\n",
        "#         return self.dataset_length\n",
        "\n",
        "#     def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "#         \"\"\"\n",
        "#         Load image from image_path of given dataset_path and convert into input_tensor and labels\n",
        "#         Convert gt data into input_ids (tokenized string)\n",
        "#         Returns:\n",
        "#             input_tensor : preprocessed image\n",
        "#             input_ids : tokenized gt_data\n",
        "#             labels : masked labels (model doesn't need to predict prompt and pad token)\n",
        "#         \"\"\"\n",
        "#         sample = self.dataset[idx]\n",
        "\n",
        "#         # pixel values (we remove the batch dimension)\n",
        "#         pixel_values = processor(sample[\"image\"].convert(\"RGB\"), random_padding=self.split == \"train\", return_tensors=\"pt\").pixel_values\n",
        "#         pixel_values = pixel_values.squeeze()\n",
        "\n",
        "#         # labels, which are the input ids of the target sequence\n",
        "#         target_sequence = random.choice(self.gt_token_sequences[idx])  # can be more than one, e.g., DocVQA Task 1\n",
        "#         input_ids = processor.tokenizer(\n",
        "#             target_sequence,\n",
        "#             add_special_tokens=False,\n",
        "#             max_length=self.max_length,\n",
        "#             padding=\"max_length\",\n",
        "#             truncation=True,\n",
        "#             return_tensors=\"pt\",\n",
        "#         )[\"input_ids\"].squeeze(0)\n",
        "\n",
        "#         labels = input_ids.clone()\n",
        "#         labels[labels == processor.tokenizer.pad_token_id] = self.ignore_id  # model doesn't need to predict pad token\n",
        "#         # labels[: torch.nonzero(labels == self.prompt_end_token_id).sum() + 1] = self.ignore_id  # model doesn't need to predict prompt (for VQA)\n",
        "        \n",
        "#         encoding = dict(pixel_values=pixel_values,\n",
        "#                         labels=labels)\n",
        "        \n",
        "#         return encoding"
      ],
      "metadata": {
        "id": "qsdbbCQlAbj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DonutDataset(\"nielsr/rvl_cdip_10_examples_per_class_donut\", max_length=max_length,\n",
        "                             split=\"train\", task_start_token=\"<s_rvlcdip>\", prompt_end_token=\"<s_rvlcdip>\",\n",
        "                             sort_json_key=False, # rvlcdip dataset is preprocessed, so no need for this\n",
        "                             )"
      ],
      "metadata": {
        "id": "KweqMg2jAewE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# I'm using a small batch size to make sure it fits in the memory Colab provides\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "H_YGg87vAhaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "print(batch.keys())"
      ],
      "metadata": {
        "id": "n5HVHfzAAkJP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}